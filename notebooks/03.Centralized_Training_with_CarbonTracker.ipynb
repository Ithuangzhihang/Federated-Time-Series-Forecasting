{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1965fde6",
   "metadata": {},
   "source": [
    "### In this notebook we perform Centralized learning, almost like 02.Individual_Training. \n",
    "\n",
    "In centralized learning there is an entity that has access to data from all base stations.\n",
    "Here, there is no option to filter out any base station.\n",
    "In this setting we also measure the energy consumption using the Carbontracker tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c11879af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "parent = Path(os.path.abspath(\"\")).resolve().parents[0]\n",
    "if parent not in sys.path:\n",
    "    sys.path.insert(0, str(parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41f6cbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d264d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.utils.data_utils import read_data, generate_time_lags, time_to_feature, handle_nans, to_Xy, \\\n",
    "    to_torch_dataset, to_timeseries_rep, assign_statistics, \\\n",
    "    to_train_val, scale_features, get_data_by_area, remove_identifiers, get_exogenous_data_by_area, handle_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48559200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.utils.train_utils import train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67c5e96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.models.mlp import MLP\n",
    "from ml.models.rnn import RNN\n",
    "from ml.models.lstm import LSTM\n",
    "from ml.models.gru import GRU\n",
    "from ml.models.cnn import CNN\n",
    "from ml.models.rnn_autoencoder import DualAttentionAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81e60d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    data_path='../dataset/full_dataset.csv', # dataset\n",
    "    data_path_test=['../dataset/ElBorn_test.csv'], # test dataset\n",
    "    test_size=0.2, # validation size \n",
    "    targets=['rnti_count', 'rb_down', 'rb_up', 'down', 'up'], # the target columns\n",
    "    num_lags=10, # the number of past observations to feed as input\n",
    "\n",
    "    \n",
    "    filter_bs=None, # whether to use a single bs for training. It will be changed dynamically\n",
    "    identifier='District', # the column name that identifies a bs\n",
    "\n",
    "    nan_constant=0, # the constant to transform nan values\n",
    "    x_scaler='minmax', # x_scaler\n",
    "    y_scaler='minmax', # y_scaler\n",
    "    outlier_detection=None, # whether to perform flooring and capping\n",
    "\n",
    "    \n",
    "    criterion='mse', # optimization criterion, mse or l1\n",
    "    epochs=150, # the number of maximum epochs\n",
    "    lr=0.001, # learning rate\n",
    "    optimizer='adam', # the optimizer, it can be sgd or adam\n",
    "    batch_size=128, # the batch size to use\n",
    "    early_stopping=True, # whether to use early stopping\n",
    "    patience=50, # patience value for the early stopping parameter (if specified)\n",
    "    max_grad_norm=0.0, # whether to clip grad norm\n",
    "    reg1=0.0, # l1 regularization\n",
    "    reg2=0.0, # l2 regularization\n",
    "    \n",
    "    plot_history=True, # plot loss history\n",
    "\n",
    "    cuda=True, # whether to use gpu\n",
    "    \n",
    "    seed=0, # reproducibility\n",
    "\n",
    "    assign_stats=None, # whether to use statistics as exogenous data, [\"mean\", \"median\", \"std\", \"variance\", \"kurtosis\", \"skew\"]\n",
    "    use_time_features=False # whether to use datetime features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0660fe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script arguments: Namespace(assign_stats=None, batch_size=128, criterion='mse', cuda=True, data_path='../dataset/full_dataset.csv', data_path_test=['../dataset/ElBorn_test.csv'], early_stopping=True, epochs=150, filter_bs=None, identifier='District', lr=0.001, max_grad_norm=0.0, nan_constant=0, num_lags=10, optimizer='adam', outlier_detection=None, patience=50, plot_history=True, reg1=0.0, reg2=0.0, seed=0, targets=['rnti_count', 'rb_down', 'rb_up', 'down', 'up'], test_size=0.2, use_time_features=False, x_scaler='minmax', y_scaler='minmax')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Script arguments: {args}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e574ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if args.cuda and torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26b20d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection specification\n",
    "if args.outlier_detection is not None:\n",
    "    outlier_columns = ['rb_down', 'rb_up', 'down', 'up']\n",
    "    outlier_kwargs = {\"ElBorn\": (10, 90), \"LesCorts\": (10, 90), \"PobleSec\": (5, 95)}\n",
    "    args.outlier_columns = outlier_columns\n",
    "    args.outlier_kwargs = outlier_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49d661a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all():\n",
    "    # ensure reproducibility\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3b0bafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebd59b3",
   "metadata": {},
   "source": [
    "### By setting filter_bs to None, the preprocessing pipeline returns data from all three base stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acf0480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preprocessing(filter_bs=None):\n",
    "    \"\"\"Preprocess a given .csv\"\"\"\n",
    "    # read data\n",
    "    df = read_data(args.data_path, filter_data=filter_bs)\n",
    "    # handle nans\n",
    "    df = handle_nans(train_data=df, constant=args.nan_constant,\n",
    "                     identifier=args.identifier)\n",
    "    # split to train/validation\n",
    "    train_data, val_data = to_train_val(df)\n",
    "    \n",
    "    # handle outliers (if specified)\n",
    "    if args.outlier_detection is not None:\n",
    "        train_data = handle_outliers(df=train_data, columns=args.outlier_columns,\n",
    "                                     identifier=args.identifier, kwargs=args.outlier_kwargs)\n",
    "    \n",
    "    # get X and y\n",
    "    X_train, X_val, y_train, y_val = to_Xy(train_data=train_data, val_data=val_data,\n",
    "                                          targets=args.targets)\n",
    "    \n",
    "    # scale X\n",
    "    X_train, X_val, x_scaler = scale_features(train_data=X_train, val_data=X_val,\n",
    "                                             scaler=args.x_scaler, identifier=args.identifier)\n",
    "    # scale y\n",
    "    y_train, y_val, y_scaler = scale_features(train_data=y_train, val_data=y_val,\n",
    "                                             scaler=args.y_scaler, identifier=args.identifier)\n",
    "    \n",
    "    # generate time lags\n",
    "    X_train = generate_time_lags(X_train, args.num_lags)\n",
    "    X_val = generate_time_lags(X_val, args.num_lags)\n",
    "    y_train = generate_time_lags(y_train, args.num_lags, is_y=True)\n",
    "    y_val = generate_time_lags(y_val, args.num_lags, is_y=True)\n",
    "    \n",
    "    # get datetime features as exogenous data\n",
    "    date_time_df_train = time_to_feature(\n",
    "        X_train, args.use_time_features, identifier=args.identifier\n",
    "    )\n",
    "    date_time_df_val = time_to_feature(\n",
    "        X_val, args.use_time_features, identifier=args.identifier\n",
    "    )\n",
    "    \n",
    "    # get statistics as exogenous data\n",
    "    stats_df_train = assign_statistics(X_train, args.assign_stats, args.num_lags,\n",
    "                                       targets=args.targets, identifier=args.identifier)\n",
    "    stats_df_val = assign_statistics(X_val, args.assign_stats, args.num_lags, \n",
    "                                       targets=args.targets, identifier=args.identifier)\n",
    "    \n",
    "    # concat the exogenous features (if any) to a single dataframe\n",
    "    if date_time_df_train is not None or stats_df_train is not None:\n",
    "        exogenous_data_train = pd.concat([date_time_df_train, stats_df_train], axis=1)\n",
    "        # remove duplicate columns (if any)\n",
    "        exogenous_data_train = exogenous_data_train.loc[:, ~exogenous_data_train.columns.duplicated()].copy()\n",
    "        assert len(exogenous_data_train) == len(X_train) == len(y_train)\n",
    "    else:\n",
    "        exogenous_data_train = None\n",
    "    if date_time_df_val is not None or stats_df_val is not None:\n",
    "        exogenous_data_val = pd.concat([date_time_df_val, stats_df_val], axis=1)\n",
    "        exogenous_data_val = exogenous_data_val.loc[:, ~exogenous_data_val.columns.duplicated()].copy()\n",
    "        assert len(exogenous_data_val) == len(X_val) == len(y_val)\n",
    "    else:\n",
    "        exogenous_data_val = None\n",
    "        \n",
    "    return X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scaler, y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85eb3771",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO logger 2024-05-23 17:01:35,024 | data_utils.py:389 | Observations info in ElBorn\n",
      "INFO logger 2024-05-23 17:01:35,025 | data_utils.py:390 | \tTotal number of samples:  4192\n",
      "INFO logger 2024-05-23 17:01:35,025 | data_utils.py:391 | \tNumber of samples for training: 3354\n",
      "INFO logger 2024-05-23 17:01:35,025 | data_utils.py:392 | \tNumber of samples for validation:  838\n",
      "INFO logger 2024-05-23 17:01:35,026 | data_utils.py:389 | Observations info in LesCorts\n",
      "INFO logger 2024-05-23 17:01:35,027 | data_utils.py:390 | \tTotal number of samples:  6892\n",
      "INFO logger 2024-05-23 17:01:35,027 | data_utils.py:391 | \tNumber of samples for training: 5514\n",
      "INFO logger 2024-05-23 17:01:35,027 | data_utils.py:392 | \tNumber of samples for validation:  1378\n",
      "INFO logger 2024-05-23 17:01:35,028 | data_utils.py:389 | Observations info in PobleSec\n",
      "INFO logger 2024-05-23 17:01:35,029 | data_utils.py:390 | \tTotal number of samples:  15927\n",
      "INFO logger 2024-05-23 17:01:35,029 | data_utils.py:391 | \tNumber of samples for training: 12742\n",
      "INFO logger 2024-05-23 17:01:35,029 | data_utils.py:392 | \tNumber of samples for validation:  3185\n",
      "INFO logger 2024-05-23 17:01:35,030 | data_utils.py:395 | Observations info using all data\n",
      "INFO logger 2024-05-23 17:01:35,030 | data_utils.py:396 | \tTotal number of samples:  27011\n",
      "INFO logger 2024-05-23 17:01:35,030 | data_utils.py:397 | \tNumber of samples for training: 21610\n",
      "INFO logger 2024-05-23 17:01:35,030 | data_utils.py:398 | \tNumber of samples for validation:  5401\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scaler, y_scaler = make_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45e64887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rb_up_var_lag-10</th>\n",
       "      <th>rb_up_lag-10</th>\n",
       "      <th>rb_down_var_lag-10</th>\n",
       "      <th>rb_down_lag-10</th>\n",
       "      <th>mcs_up_var_lag-10</th>\n",
       "      <th>mcs_up_lag-10</th>\n",
       "      <th>mcs_down_var_lag-10</th>\n",
       "      <th>mcs_down_lag-10</th>\n",
       "      <th>rnti_count_lag-10</th>\n",
       "      <th>up_lag-10</th>\n",
       "      <th>...</th>\n",
       "      <th>rb_down_var_lag-1</th>\n",
       "      <th>rb_down_lag-1</th>\n",
       "      <th>mcs_up_var_lag-1</th>\n",
       "      <th>mcs_up_lag-1</th>\n",
       "      <th>mcs_down_var_lag-1</th>\n",
       "      <th>mcs_down_lag-1</th>\n",
       "      <th>rnti_count_lag-1</th>\n",
       "      <th>up_lag-1</th>\n",
       "      <th>down_lag-1</th>\n",
       "      <th>District</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:16:00</th>\n",
       "      <td>3.143298e-08</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>4.497698e-08</td>\n",
       "      <td>0.040119</td>\n",
       "      <td>0.207425</td>\n",
       "      <td>0.483274</td>\n",
       "      <td>0.864261</td>\n",
       "      <td>0.937978</td>\n",
       "      <td>0.233939</td>\n",
       "      <td>0.001755</td>\n",
       "      <td>...</td>\n",
       "      <td>4.711435e-08</td>\n",
       "      <td>0.048765</td>\n",
       "      <td>0.232918</td>\n",
       "      <td>0.478785</td>\n",
       "      <td>0.862283</td>\n",
       "      <td>0.926591</td>\n",
       "      <td>0.281372</td>\n",
       "      <td>0.002074</td>\n",
       "      <td>0.091860</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:18:00</th>\n",
       "      <td>4.439640e-08</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>4.615535e-08</td>\n",
       "      <td>0.048621</td>\n",
       "      <td>0.259314</td>\n",
       "      <td>0.530084</td>\n",
       "      <td>0.864602</td>\n",
       "      <td>0.924797</td>\n",
       "      <td>0.279543</td>\n",
       "      <td>0.002709</td>\n",
       "      <td>...</td>\n",
       "      <td>4.562575e-08</td>\n",
       "      <td>0.048478</td>\n",
       "      <td>0.242482</td>\n",
       "      <td>0.499756</td>\n",
       "      <td>0.855495</td>\n",
       "      <td>0.931433</td>\n",
       "      <td>0.279886</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.091507</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:20:00</th>\n",
       "      <td>2.993595e-08</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>4.646104e-08</td>\n",
       "      <td>0.044267</td>\n",
       "      <td>0.261772</td>\n",
       "      <td>0.512427</td>\n",
       "      <td>0.864957</td>\n",
       "      <td>0.930847</td>\n",
       "      <td>0.255049</td>\n",
       "      <td>0.001829</td>\n",
       "      <td>...</td>\n",
       "      <td>4.634120e-08</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.241381</td>\n",
       "      <td>0.450879</td>\n",
       "      <td>0.867035</td>\n",
       "      <td>0.929728</td>\n",
       "      <td>0.274900</td>\n",
       "      <td>0.001903</td>\n",
       "      <td>0.089721</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:22:00</th>\n",
       "      <td>5.382563e-08</td>\n",
       "      <td>0.002076</td>\n",
       "      <td>4.532153e-08</td>\n",
       "      <td>0.055921</td>\n",
       "      <td>0.320280</td>\n",
       "      <td>0.506925</td>\n",
       "      <td>0.854759</td>\n",
       "      <td>0.925932</td>\n",
       "      <td>0.321098</td>\n",
       "      <td>0.002827</td>\n",
       "      <td>...</td>\n",
       "      <td>4.689734e-08</td>\n",
       "      <td>0.056978</td>\n",
       "      <td>0.315197</td>\n",
       "      <td>0.495057</td>\n",
       "      <td>0.876713</td>\n",
       "      <td>0.927495</td>\n",
       "      <td>0.322424</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.107071</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:24:00</th>\n",
       "      <td>5.922178e-08</td>\n",
       "      <td>0.002290</td>\n",
       "      <td>4.655542e-08</td>\n",
       "      <td>0.060925</td>\n",
       "      <td>0.286799</td>\n",
       "      <td>0.497228</td>\n",
       "      <td>0.854279</td>\n",
       "      <td>0.929208</td>\n",
       "      <td>0.348702</td>\n",
       "      <td>0.003109</td>\n",
       "      <td>...</td>\n",
       "      <td>4.516392e-08</td>\n",
       "      <td>0.039556</td>\n",
       "      <td>0.267656</td>\n",
       "      <td>0.452835</td>\n",
       "      <td>0.861872</td>\n",
       "      <td>0.928728</td>\n",
       "      <td>0.232544</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>0.075132</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     rb_up_var_lag-10  rb_up_lag-10  rb_down_var_lag-10  \\\n",
       "time                                                                      \n",
       "2018-03-28 16:16:00      3.143298e-08      0.001213        4.497698e-08   \n",
       "2018-03-28 16:18:00      4.439640e-08      0.001912        4.615535e-08   \n",
       "2018-03-28 16:20:00      2.993595e-08      0.001362        4.646104e-08   \n",
       "2018-03-28 16:22:00      5.382563e-08      0.002076        4.532153e-08   \n",
       "2018-03-28 16:24:00      5.922178e-08      0.002290        4.655542e-08   \n",
       "\n",
       "                     rb_down_lag-10  mcs_up_var_lag-10  mcs_up_lag-10  \\\n",
       "time                                                                    \n",
       "2018-03-28 16:16:00        0.040119           0.207425       0.483274   \n",
       "2018-03-28 16:18:00        0.048621           0.259314       0.530084   \n",
       "2018-03-28 16:20:00        0.044267           0.261772       0.512427   \n",
       "2018-03-28 16:22:00        0.055921           0.320280       0.506925   \n",
       "2018-03-28 16:24:00        0.060925           0.286799       0.497228   \n",
       "\n",
       "                     mcs_down_var_lag-10  mcs_down_lag-10  rnti_count_lag-10  \\\n",
       "time                                                                           \n",
       "2018-03-28 16:16:00             0.864261         0.937978           0.233939   \n",
       "2018-03-28 16:18:00             0.864602         0.924797           0.279543   \n",
       "2018-03-28 16:20:00             0.864957         0.930847           0.255049   \n",
       "2018-03-28 16:22:00             0.854759         0.925932           0.321098   \n",
       "2018-03-28 16:24:00             0.854279         0.929208           0.348702   \n",
       "\n",
       "                     up_lag-10  ...  rb_down_var_lag-1  rb_down_lag-1  \\\n",
       "time                            ...                                     \n",
       "2018-03-28 16:16:00   0.001755  ...       4.711435e-08       0.048765   \n",
       "2018-03-28 16:18:00   0.002709  ...       4.562575e-08       0.048478   \n",
       "2018-03-28 16:20:00   0.001829  ...       4.634120e-08       0.047619   \n",
       "2018-03-28 16:22:00   0.002827  ...       4.689734e-08       0.056978   \n",
       "2018-03-28 16:24:00   0.003109  ...       4.516392e-08       0.039556   \n",
       "\n",
       "                     mcs_up_var_lag-1  mcs_up_lag-1  mcs_down_var_lag-1  \\\n",
       "time                                                                      \n",
       "2018-03-28 16:16:00          0.232918      0.478785            0.862283   \n",
       "2018-03-28 16:18:00          0.242482      0.499756            0.855495   \n",
       "2018-03-28 16:20:00          0.241381      0.450879            0.867035   \n",
       "2018-03-28 16:22:00          0.315197      0.495057            0.876713   \n",
       "2018-03-28 16:24:00          0.267656      0.452835            0.861872   \n",
       "\n",
       "                     mcs_down_lag-1  rnti_count_lag-1  up_lag-1  down_lag-1  \\\n",
       "time                                                                          \n",
       "2018-03-28 16:16:00        0.926591          0.281372  0.002074    0.091860   \n",
       "2018-03-28 16:18:00        0.931433          0.279886  0.002412    0.091507   \n",
       "2018-03-28 16:20:00        0.929728          0.274900  0.001903    0.089721   \n",
       "2018-03-28 16:22:00        0.927495          0.322424  0.003139    0.107071   \n",
       "2018-03-28 16:24:00        0.928728          0.232544  0.001653    0.075132   \n",
       "\n",
       "                     District  \n",
       "time                           \n",
       "2018-03-28 16:16:00    ElBorn  \n",
       "2018-03-28 16:18:00    ElBorn  \n",
       "2018-03-28 16:20:00    ElBorn  \n",
       "2018-03-28 16:22:00    ElBorn  \n",
       "2018-03-28 16:24:00    ElBorn  \n",
       "\n",
       "[5 rows x 111 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "591b150f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rnti_count</th>\n",
       "      <th>rb_down</th>\n",
       "      <th>rb_up</th>\n",
       "      <th>down</th>\n",
       "      <th>up</th>\n",
       "      <th>District</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:16:00</th>\n",
       "      <td>0.279886</td>\n",
       "      <td>0.048478</td>\n",
       "      <td>0.001722</td>\n",
       "      <td>0.091507</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:18:00</th>\n",
       "      <td>0.274900</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.089721</td>\n",
       "      <td>0.001903</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:20:00</th>\n",
       "      <td>0.322424</td>\n",
       "      <td>0.056978</td>\n",
       "      <td>0.002261</td>\n",
       "      <td>0.107071</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:22:00</th>\n",
       "      <td>0.232544</td>\n",
       "      <td>0.039556</td>\n",
       "      <td>0.001340</td>\n",
       "      <td>0.075132</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:24:00</th>\n",
       "      <td>0.313391</td>\n",
       "      <td>0.054345</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>0.102806</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     rnti_count   rb_down     rb_up      down        up  \\\n",
       "time                                                                      \n",
       "2018-03-28 16:16:00    0.279886  0.048478  0.001722  0.091507  0.002412   \n",
       "2018-03-28 16:18:00    0.274900  0.047619  0.001502  0.089721  0.001903   \n",
       "2018-03-28 16:20:00    0.322424  0.056978  0.002261  0.107071  0.003139   \n",
       "2018-03-28 16:22:00    0.232544  0.039556  0.001340  0.075132  0.001653   \n",
       "2018-03-28 16:24:00    0.313391  0.054345  0.001927  0.102806  0.002580   \n",
       "\n",
       "                    District  \n",
       "time                          \n",
       "2018-03-28 16:16:00   ElBorn  \n",
       "2018-03-28 16:18:00   ElBorn  \n",
       "2018-03-28 16:20:00   ElBorn  \n",
       "2018-03-28 16:22:00   ElBorn  \n",
       "2018-03-28 16:24:00   ElBorn  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0fa465a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MinMaxScaler(), MinMaxScaler())"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaler, y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55102f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_postprocessing(X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scaler, y_scaler):\n",
    "    \"\"\"Make data ready to be fed into ml algorithms\"\"\"\n",
    "    # if there are more than one specified areas, get the data per area\n",
    "    if X_train[args.identifier].nunique() != 1:\n",
    "        area_X_train, area_X_val, area_y_train, area_y_val = get_data_by_area(X_train, X_val,\n",
    "                                                                              y_train, y_val, \n",
    "                                                                              identifier=args.identifier)\n",
    "    else:\n",
    "        area_X_train, area_X_val, area_y_train, area_y_val = None, None, None, None\n",
    "\n",
    "    # Get the exogenous data per area.\n",
    "    if exogenous_data_train is not None:\n",
    "        exogenous_data_train, exogenous_data_val = get_exogenous_data_by_area(exogenous_data_train,\n",
    "                                                                              exogenous_data_val)\n",
    "    # transform to np\n",
    "    if area_X_train is not None:\n",
    "        for area in area_X_train:\n",
    "            tmp_X_train, tmp_y_train, tmp_X_val, tmp_y_val = remove_identifiers(\n",
    "                area_X_train[area], area_y_train[area], area_X_val[area], area_y_val[area])\n",
    "            tmp_X_train, tmp_y_train = tmp_X_train.to_numpy(), tmp_y_train.to_numpy()\n",
    "            tmp_X_val, tmp_y_val = tmp_X_val.to_numpy(), tmp_y_val.to_numpy()\n",
    "            area_X_train[area] = tmp_X_train\n",
    "            area_X_val[area] = tmp_X_val\n",
    "            area_y_train[area] = tmp_y_train\n",
    "            area_y_val[area] = tmp_y_val\n",
    "    \n",
    "    if exogenous_data_train is not None:\n",
    "        for area in exogenous_data_train:\n",
    "            exogenous_data_train[area] = exogenous_data_train[area].to_numpy()\n",
    "            exogenous_data_val[area] = exogenous_data_val[area].to_numpy()\n",
    "    \n",
    "    # remove identifiers from features, targets\n",
    "    X_train, y_train, X_val, y_val = remove_identifiers(X_train, y_train, X_val, y_val)\n",
    "    assert len(X_train.columns) == len(X_val.columns)\n",
    "    \n",
    "    num_features = len(X_train.columns) // args.num_lags\n",
    "    \n",
    "    # to timeseries representation\n",
    "    X_train = to_timeseries_rep(X_train.to_numpy(), num_lags=args.num_lags,\n",
    "                                            num_features=num_features)\n",
    "    X_val = to_timeseries_rep(X_val.to_numpy(), num_lags=args.num_lags,\n",
    "                                          num_features=num_features)\n",
    "    \n",
    "    if area_X_train is not None:\n",
    "        area_X_train = to_timeseries_rep(area_X_train, num_lags=args.num_lags,\n",
    "                                                     num_features=num_features)\n",
    "        area_X_val = to_timeseries_rep(area_X_val, num_lags=args.num_lags,\n",
    "                                                   num_features=num_features)\n",
    "    \n",
    "    # transform targets to numpy\n",
    "    y_train, y_val = y_train.to_numpy(), y_val.to_numpy()\n",
    "    \n",
    "    # centralized (all) learning specific\n",
    "    if not args.filter_bs and exogenous_data_train is not None:\n",
    "        exogenous_data_train_combined, exogenous_data_val_combined = [], []\n",
    "        for area in exogenous_data_train:\n",
    "            exogenous_data_train_combined.extend(exogenous_data_train[area])\n",
    "            exogenous_data_val_combined.extend(exogenous_data_val[area])\n",
    "        exogenous_data_train_combined = np.stack(exogenous_data_train_combined)\n",
    "        exogenous_data_val_combined = np.stack(exogenous_data_val_combined)\n",
    "        exogenous_data_train[\"all\"] = exogenous_data_train_combined\n",
    "        exogenous_data_val[\"all\"] = exogenous_data_val_combined\n",
    "    return X_train, X_val, y_train, y_val, area_X_train, area_X_val, area_y_train, area_y_val, exogenous_data_train, exogenous_data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698aa32c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce3616bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val, area_X_train, area_X_val, area_y_train, area_y_val, exogenous_data_train, exogenous_data_val = make_postprocessing(X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scaler, y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6592db61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ElBorn', 'LesCorts', 'PobleSec'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "area_X_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d521ab70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ElBorn', 'LesCorts', 'PobleSec'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "area_X_val.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c90c8a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[3.14329753e-08],\n",
       "         [1.21277163e-03],\n",
       "         [4.49769750e-08],\n",
       "         [4.01188992e-02],\n",
       "         [2.07425252e-01],\n",
       "         [4.83274072e-01],\n",
       "         [8.64260554e-01],\n",
       "         [9.37977910e-01],\n",
       "         [2.33939394e-01],\n",
       "         [1.75528484e-03],\n",
       "         [7.64968917e-02]],\n",
       "\n",
       "        [[4.43963977e-08],\n",
       "         [1.91187230e-03],\n",
       "         [4.61553498e-08],\n",
       "         [4.86211926e-02],\n",
       "         [2.59314001e-01],\n",
       "         [5.30084312e-01],\n",
       "         [8.64602447e-01],\n",
       "         [9.24797297e-01],\n",
       "         [2.79542595e-01],\n",
       "         [2.70937034e-03],\n",
       "         [9.14471596e-02]],\n",
       "\n",
       "        [[2.99359542e-08],\n",
       "         [1.36231177e-03],\n",
       "         [4.64610359e-08],\n",
       "         [4.42666970e-02],\n",
       "         [2.61771828e-01],\n",
       "         [5.12427032e-01],\n",
       "         [8.64957333e-01],\n",
       "         [9.30847228e-01],\n",
       "         [2.55048603e-01],\n",
       "         [1.82946306e-03],\n",
       "         [8.37529153e-02]],\n",
       "\n",
       "        [[5.38256302e-08],\n",
       "         [2.07580579e-03],\n",
       "         [4.53215314e-08],\n",
       "         [5.59207276e-02],\n",
       "         [3.20279956e-01],\n",
       "         [5.06924510e-01],\n",
       "         [8.54759097e-01],\n",
       "         [9.25932348e-01],\n",
       "         [3.21097761e-01],\n",
       "         [2.82748532e-03],\n",
       "         [1.05646886e-01]],\n",
       "\n",
       "        [[5.92217795e-08],\n",
       "         [2.28964840e-03],\n",
       "         [4.65554244e-08],\n",
       "         [6.09245785e-02],\n",
       "         [2.86799014e-01],\n",
       "         [4.97228354e-01],\n",
       "         [8.54279280e-01],\n",
       "         [9.29207981e-01],\n",
       "         [3.48702133e-01],\n",
       "         [3.10886209e-03],\n",
       "         [1.15539603e-01]],\n",
       "\n",
       "        [[4.27273470e-08],\n",
       "         [1.97860459e-03],\n",
       "         [4.64907401e-08],\n",
       "         [5.53930141e-02],\n",
       "         [3.06054235e-01],\n",
       "         [5.20298362e-01],\n",
       "         [8.64346623e-01],\n",
       "         [9.37814415e-01],\n",
       "         [3.20914805e-01],\n",
       "         [2.70142243e-03],\n",
       "         [1.05143227e-01]],\n",
       "\n",
       "        [[3.29200631e-08],\n",
       "         [1.46437297e-03],\n",
       "         [4.62307348e-08],\n",
       "         [4.84285802e-02],\n",
       "         [2.37825811e-01],\n",
       "         [4.55921143e-01],\n",
       "         [8.78899217e-01],\n",
       "         [9.18324053e-01],\n",
       "         [2.80411661e-01],\n",
       "         [1.78931502e-03],\n",
       "         [9.16718319e-02]],\n",
       "\n",
       "        [[4.18195008e-08],\n",
       "         [1.60662318e-03],\n",
       "         [4.63657486e-08],\n",
       "         [5.45288548e-02],\n",
       "         [2.72135526e-01],\n",
       "         [4.97247547e-01],\n",
       "         [8.55873585e-01],\n",
       "         [9.23164129e-01],\n",
       "         [3.14328194e-01],\n",
       "         [2.19081831e-03],\n",
       "         [1.03006192e-01]],\n",
       "\n",
       "        [[3.44121815e-08],\n",
       "         [1.69989897e-03],\n",
       "         [4.60645957e-08],\n",
       "         [4.88040037e-02],\n",
       "         [2.65302420e-01],\n",
       "         [4.96077031e-01],\n",
       "         [8.66527557e-01],\n",
       "         [9.23408091e-01],\n",
       "         [2.80640364e-01],\n",
       "         [2.17916491e-03],\n",
       "         [9.25039276e-02]],\n",
       "\n",
       "        [[3.66808699e-08],\n",
       "         [1.56419107e-03],\n",
       "         [4.71143515e-08],\n",
       "         [4.87649143e-02],\n",
       "         [2.32917622e-01],\n",
       "         [4.78784889e-01],\n",
       "         [8.62282574e-01],\n",
       "         [9.26590562e-01],\n",
       "         [2.81372219e-01],\n",
       "         [2.07389053e-03],\n",
       "         [9.18597132e-02]]],\n",
       "\n",
       "\n",
       "       [[[4.43963977e-08],\n",
       "         [1.91187230e-03],\n",
       "         [4.61553498e-08],\n",
       "         [4.86211926e-02],\n",
       "         [2.59314001e-01],\n",
       "         [5.30084312e-01],\n",
       "         [8.64602447e-01],\n",
       "         [9.24797297e-01],\n",
       "         [2.79542595e-01],\n",
       "         [2.70937034e-03],\n",
       "         [9.14471596e-02]],\n",
       "\n",
       "        [[2.99359542e-08],\n",
       "         [1.36231177e-03],\n",
       "         [4.64610359e-08],\n",
       "         [4.42666970e-02],\n",
       "         [2.61771828e-01],\n",
       "         [5.12427032e-01],\n",
       "         [8.64957333e-01],\n",
       "         [9.30847228e-01],\n",
       "         [2.55048603e-01],\n",
       "         [1.82946306e-03],\n",
       "         [8.37529153e-02]],\n",
       "\n",
       "        [[5.38256302e-08],\n",
       "         [2.07580579e-03],\n",
       "         [4.53215314e-08],\n",
       "         [5.59207276e-02],\n",
       "         [3.20279956e-01],\n",
       "         [5.06924510e-01],\n",
       "         [8.54759097e-01],\n",
       "         [9.25932348e-01],\n",
       "         [3.21097761e-01],\n",
       "         [2.82748532e-03],\n",
       "         [1.05646886e-01]],\n",
       "\n",
       "        [[5.92217795e-08],\n",
       "         [2.28964840e-03],\n",
       "         [4.65554244e-08],\n",
       "         [6.09245785e-02],\n",
       "         [2.86799014e-01],\n",
       "         [4.97228354e-01],\n",
       "         [8.54279280e-01],\n",
       "         [9.29207981e-01],\n",
       "         [3.48702133e-01],\n",
       "         [3.10886209e-03],\n",
       "         [1.15539603e-01]],\n",
       "\n",
       "        [[4.27273470e-08],\n",
       "         [1.97860459e-03],\n",
       "         [4.64907401e-08],\n",
       "         [5.53930141e-02],\n",
       "         [3.06054235e-01],\n",
       "         [5.20298362e-01],\n",
       "         [8.64346623e-01],\n",
       "         [9.37814415e-01],\n",
       "         [3.20914805e-01],\n",
       "         [2.70142243e-03],\n",
       "         [1.05143227e-01]],\n",
       "\n",
       "        [[3.29200631e-08],\n",
       "         [1.46437297e-03],\n",
       "         [4.62307348e-08],\n",
       "         [4.84285802e-02],\n",
       "         [2.37825811e-01],\n",
       "         [4.55921143e-01],\n",
       "         [8.78899217e-01],\n",
       "         [9.18324053e-01],\n",
       "         [2.80411661e-01],\n",
       "         [1.78931502e-03],\n",
       "         [9.16718319e-02]],\n",
       "\n",
       "        [[4.18195008e-08],\n",
       "         [1.60662318e-03],\n",
       "         [4.63657486e-08],\n",
       "         [5.45288548e-02],\n",
       "         [2.72135526e-01],\n",
       "         [4.97247547e-01],\n",
       "         [8.55873585e-01],\n",
       "         [9.23164129e-01],\n",
       "         [3.14328194e-01],\n",
       "         [2.19081831e-03],\n",
       "         [1.03006192e-01]],\n",
       "\n",
       "        [[3.44121815e-08],\n",
       "         [1.69989897e-03],\n",
       "         [4.60645957e-08],\n",
       "         [4.88040037e-02],\n",
       "         [2.65302420e-01],\n",
       "         [4.96077031e-01],\n",
       "         [8.66527557e-01],\n",
       "         [9.23408091e-01],\n",
       "         [2.80640364e-01],\n",
       "         [2.17916491e-03],\n",
       "         [9.25039276e-02]],\n",
       "\n",
       "        [[3.66808699e-08],\n",
       "         [1.56419107e-03],\n",
       "         [4.71143515e-08],\n",
       "         [4.87649143e-02],\n",
       "         [2.32917622e-01],\n",
       "         [4.78784889e-01],\n",
       "         [8.62282574e-01],\n",
       "         [9.26590562e-01],\n",
       "         [2.81372219e-01],\n",
       "         [2.07389053e-03],\n",
       "         [9.18597132e-02]],\n",
       "\n",
       "        [[3.66673198e-08],\n",
       "         [1.72214315e-03],\n",
       "         [4.56257538e-08],\n",
       "         [4.84776907e-02],\n",
       "         [2.42481545e-01],\n",
       "         [4.99756068e-01],\n",
       "         [8.55494559e-01],\n",
       "         [9.31432724e-01],\n",
       "         [2.79885650e-01],\n",
       "         [2.41222628e-03],\n",
       "         [9.15068761e-02]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a11d39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27988565, 0.04847769, 0.00172214, 0.09150688, 0.00241223],\n",
       "       [0.27489996, 0.04761904, 0.00150213, 0.08972067, 0.00190256]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "076cc206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21580, 5371)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7777e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0274244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_dims(X_train, exogenous_data_train):\n",
    "    if args.model_name == \"mlp\":\n",
    "        input_dim = X_train.shape[1] * X_train.shape[2]\n",
    "    else:\n",
    "        input_dim = X_train.shape[2]\n",
    "    \n",
    "    if exogenous_data_train is not None:\n",
    "        if len(exogenous_data_train) == 1:\n",
    "            cid = next(iter(exogenous_data_train.keys()))\n",
    "            exogenous_dim = exogenous_data_train[cid].shape[1]\n",
    "        else:\n",
    "            exogenous_dim = exogenous_data_train[\"all\"].shape[1]\n",
    "    else:\n",
    "        exogenous_dim = 0\n",
    "    \n",
    "    return input_dim, exogenous_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e90c8fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model: str,\n",
    "              input_dim: int,\n",
    "              out_dim: int,\n",
    "              lags: int = 10,\n",
    "              exogenous_dim: int = 0,\n",
    "              seed=0):\n",
    "    if model == \"mlp\":\n",
    "        model = MLP(input_dim=input_dim, layer_units=[256, 128, 64], num_outputs=out_dim)\n",
    "    elif model == \"rnn\":\n",
    "        model = RNN(input_dim=input_dim, rnn_hidden_size=128, num_rnn_layers=1, rnn_dropout=0.0,\n",
    "                    layer_units=[128], num_outputs=out_dim, matrix_rep=True, exogenous_dim=exogenous_dim)\n",
    "    elif model == \"lstm\":\n",
    "        model = LSTM(input_dim=input_dim, lstm_hidden_size=128, num_lstm_layers=1, lstm_dropout=0.0,\n",
    "                     layer_units=[128], num_outputs=out_dim, matrix_rep=True, exogenous_dim=exogenous_dim)\n",
    "    elif model == \"gru\":\n",
    "        model = GRU(input_dim=input_dim, gru_hidden_size=128, num_gru_layers=1, gru_dropout=0.0,\n",
    "                    layer_units=[128], num_outputs=out_dim, matrix_rep=True, exogenous_dim=exogenous_dim)\n",
    "    elif model == \"cnn\":\n",
    "        model = CNN(num_features=input_dim, lags=lags, exogenous_dim=exogenous_dim, out_dim=out_dim)\n",
    "    elif model == \"da_encoder_decoder\":\n",
    "        model = DualAttentionAutoEncoder(input_dim=input_dim, architecture=\"lstm\", matrix_rep=True)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Specified model is not implemented. Plese define your own model or choose one from ['mlp', 'rnn', 'lstm', 'gru', 'cnn', 'da_encoder_decoder']\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "495af4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 0\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "args.model_name = \"gru\"\n",
    "\n",
    "input_dim, exogenous_dim = get_input_dims(X_train, exogenous_data_train)\n",
    "\n",
    "print(input_dim, exogenous_dim)\n",
    "\n",
    "model = get_model(model=args.model_name,\n",
    "                  input_dim=input_dim,\n",
    "                  out_dim=y_train.shape[1],\n",
    "                  lags=args.num_lags,\n",
    "                  exogenous_dim=exogenous_dim,\n",
    "                  seed=args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1933251",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa4b5479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, X_train, y_train, X_val, y_val, \n",
    "        exogenous_data_train=None, exogenous_data_val=None, \n",
    "        idxs=[8, 3, 1, 10, 9], # the indices of our targets in X\n",
    "        log_per=1):\n",
    "    \n",
    "    # get exogenous data (if any)\n",
    "    if exogenous_data_train is not None and len(exogenous_data_train) > 1:\n",
    "        exogenous_data_train = exogenous_data_train[\"all\"]\n",
    "        exogenous_data_val = exogenous_data_val[\"all\"]\n",
    "    elif exogenous_data_train is not None and len(exogenous_data_train) == 1:\n",
    "        cid = next(iter(exogenous_data_train.keys()))\n",
    "        exogenous_data_train = exogenous_data_train[cid]\n",
    "        exogenous_data_val = exogenous_data_val[cid]\n",
    "    else:\n",
    "        exogenous_data_train = None\n",
    "        exogenous_data_val = None\n",
    "    num_features = len(X_train[0][0])\n",
    "    \n",
    "    # to torch loader\n",
    "    train_loader = to_torch_dataset(X_train, y_train,\n",
    "                                    num_lags=args.num_lags,\n",
    "                                    num_features=num_features,\n",
    "                                    exogenous_data=exogenous_data_train,\n",
    "                                    indices=idxs,\n",
    "                                    batch_size=args.batch_size, \n",
    "                                    shuffle=False)\n",
    "    val_loader = to_torch_dataset(X_val, y_val, \n",
    "                                  num_lags=args.num_lags,\n",
    "                                  num_features=num_features,\n",
    "                                  exogenous_data=exogenous_data_val,\n",
    "                                  indices=idxs,\n",
    "                                  batch_size=args.batch_size,\n",
    "                                  shuffle=False)\n",
    "    \n",
    "    # train the model\n",
    "    model = train(model, \n",
    "                  train_loader, val_loader,\n",
    "                  epochs=args.epochs,\n",
    "                  optimizer=args.optimizer, lr=args.lr,\n",
    "                  criterion=args.criterion,\n",
    "                  early_stopping=args.early_stopping,\n",
    "                  patience=args.patience,\n",
    "                  plot_history=args.plot_history, \n",
    "                  device=device, log_per=log_per,\n",
    "                  use_carbontracker=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edadc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = fit(model, X_train, y_train, X_val, y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "a39106e1a9d6d153b7400628e7589ff266b5caee5b0db427f0903be982155882"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
