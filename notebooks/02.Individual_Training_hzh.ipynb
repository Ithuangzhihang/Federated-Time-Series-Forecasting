{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8da638ca",
   "metadata": {},
   "source": [
    "### In this notebook we perform individual training.\n",
    "In individual learning each base station has access only to it's private dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c3fe1f1-1806-43a8-a467-516338fea3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "# 解释：将当前目录的父目录加入到sys.path中，这样就可以在当前目录下导入父目录的模块\n",
    "parent = Path(os.path.abspath(\"\")).resolve().parents[0]\n",
    "if parent not in sys.path:\n",
    "    sys.path.insert(0, str(parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01e170e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15abc428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解释\n",
    "from ml.utils.data_utils import read_data, generate_time_lags, time_to_feature, handle_nans, to_Xy, \\\n",
    "    to_torch_dataset, to_timeseries_rep, assign_statistics, \\\n",
    "    to_train_val, scale_features, get_data_by_area, remove_identifiers, get_exogenous_data_by_area, handle_outliers,\\\n",
    "    read_data_yl,get_data_by_area_yl, get_exogenous_data_by_area_yl, remove_identifiers_yl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "350c9d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.utils.train_utils import train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4688fbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.models.mlp import MLP\n",
    "from ml.models.rnn import RNN\n",
    "from ml.models.lstm import LSTM\n",
    "from ml.models.gru import GRU\n",
    "from ml.models.cnn import CNN\n",
    "from ml.models.rnn_autoencoder import DualAttentionAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f3db1750",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # 数据路径\n",
    "    data_path='/home/yanglin/Federated-Time-Series-Forecasting/dataset/yl/train_data.csv',  # 训练集路径\n",
    "    data_path_test='/home/yanglin/Federated-Time-Series-Forecasting/dataset/yl/test_data.csv',  # 测试集路径\n",
    "    test_size=0.2,  # 验证集大小\n",
    "    \n",
    "    # 预测目标\n",
    "    targets=['needs_charging'],  # 需要预测的列\n",
    "    \n",
    "    # 时间序列参数\n",
    "    num_lags=10,  # 输入特征的时间滞后数\n",
    "\n",
    "    # 数据处理参数\n",
    "    # identifier='vehicle_id',  # 标识车辆的列名\n",
    "    identifier='port',  # 标识港口\n",
    "    nan_constant=0,  # 替换NaN值的常数\n",
    "    x_scaler='minmax',  # 特征标准化方法\n",
    "    y_scaler='minmax',  # 目标标准化方法\n",
    "    outlier_detection=None,  # 异常值处理设置为None\n",
    "\n",
    "    # 模型训练参数\n",
    "    criterion='mse',  # 损失函数\n",
    "    epochs=150,  # 最大训练轮数\n",
    "    lr=0.001,  # 学习率\n",
    "    optimizer='adam',  # 优化器\n",
    "    batch_size=128,  # 批量大小\n",
    "    early_stopping=True,  # 是否使用早停机制\n",
    "    patience=50,  # 早停耐心值\n",
    "    max_grad_norm=0.0,  # 梯度裁剪\n",
    "    reg1=0.0,  # L1正则化\n",
    "    reg2=0.0,  # L2正则化\n",
    "    \n",
    "    # 其他设置\n",
    "    plot_history=True,  # 是否绘制训练损失图\n",
    "    cuda=True,  # 是否使用GPU\n",
    "    seed=0,  # 随机种子\n",
    "    assign_stats=None,  # 是否使用统计数据作为外生数据\n",
    "    use_time_features=True  # 是否使用时间特征\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56a743e",
   "metadata": {},
   "source": [
    "> You can define the base station to perform train on the filter_bs parameter and use it in block 12 or you can define the base station to block 12 explicitly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "763c39ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script arguments: Namespace(assign_stats=None, batch_size=128, criterion='mse', cuda=True, data_path='/home/yanglin/Federated-Time-Series-Forecasting/dataset/yl/train_data.csv', data_path_test='/home/yanglin/Federated-Time-Series-Forecasting/dataset/yl/test_data.csv', early_stopping=True, epochs=150, identifier='port', lr=0.001, max_grad_norm=0.0, nan_constant=0, num_lags=10, optimizer='adam', outlier_detection=None, patience=50, plot_history=True, reg1=0.0, reg2=0.0, seed=0, targets=['needs_charging'], test_size=0.2, use_time_features=True, x_scaler='minmax', y_scaler='minmax')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Script arguments: {args}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "da3431ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch_version 2.3.0\n",
      "args.cuda True\n",
      "torch.cuda.is_available() False\n",
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "import torch.version\n",
    "\n",
    "\n",
    "device = \"cuda\" if args.cuda and torch.cuda.is_available() else \"cpu\"\n",
    "print(\"torch_version\",torch.__version__)\n",
    "print(\"args.cuda\",args.cuda)\n",
    "print(\"torch.cuda.is_available()\",torch.cuda.is_available())\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bb4aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection specification 异常值检测:暂时不需要异常值检测因为我们已经处理过\n",
    "# if args.outlier_detection is not None:\n",
    "#     outlier_columns = ['rb_down', 'rb_up', 'down', 'up'] #需要进行异常值检测的列名\n",
    "#     outlier_kwargs = {\"ElBorn\": (10, 90), \"LesCorts\": (10, 90), \"PobleSec\": (5, 95)} #每个键是一个区域的名称，每个值是一个元组，用于指定该区域的异常值检测参数。\"ElBorn\": (10, 90)：表示对于ElBorn区域，异常值检测的阈值是10和90。\n",
    "#     args.outlier_columns = outlier_columns \n",
    "#     args.outlier_kwargs = outlier_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1ac1d35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all():\n",
    "    # ensure reproducibility 确保结果的可重复性\n",
    "    random.seed(args.seed) #设置Python标准库中的随机数生成器的种子为args.seed。这会影响使用random模块生成的所有随机数。\n",
    "    np.random.seed(args.seed) #设置NumPy库的随机数生成器的种子为args.seed。这会影响使用numpy.random模块生成的所有随机数。\n",
    "    torch.manual_seed(args.seed) #设置PyTorch库的随机数生成器的种子为args.seed。这会影响CPU上的所有PyTorch操作生成的随机数。\n",
    "    torch.cuda.manual_seed_all(args.seed) #设置所有CUDA设备（即GPU）的随机数生成器的种子为args.seed。这会影响在GPU上执行的所有PyTorch操作生成的随机数。\n",
    "    torch.backends.cudnn.deterministic = True #设置CuDNN后端为确定性模式，这意味着CuDNN将使用确定性的算法，从而确保相同的输入始终产生相同的输出。\n",
    "    torch.backends.cudnn.benchmark = False #禁用CuDNN的benchmark模式。启用benchmark模式可能会导致不同的计算选择不同的算法，从而产生不同的结果，因此禁用它可以确保结果的一致性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ea3ddd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00543376",
   "metadata": {},
   "source": [
    "### The preprocessing pipeline performed here for the base station specified in filter_bs argument\n",
    "Preprocessing inlcudes:\n",
    "1. NaNs Handling NANs处理\n",
    "2. Outliers Handling 异常值处理\n",
    "3. Scaling Data 数据缩放\n",
    "4. Generating time lags 生成时间滞后\n",
    "5. Generating and importing exogenous data as features (time, statistics) (if applied) 生成和导入外生数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f798b68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_csv(\"/home/yanglin/Federated-Time-Series-Forecasting/dataset/yl/train_data.csv\")\n",
    "\n",
    "# 只选择连续型数据特征\n",
    "continuous_features = ['position_x', 'position_y', 'position_z', 'heading', 'speed','speed_command', 'mileage_distance', 'power_on_time', \n",
    "                       'soc', 'fuel_level', 'avg_speed_5min']\n",
    "\n",
    "# 使用SimpleImputer填充NaN值\n",
    "imputer = SimpleImputer(strategy='mean')  # 可以根据需要选择'mean', 'median', 'most_frequent', 'constant'\n",
    "imputed_data = imputer.fit_transform(data[continuous_features])\n",
    "\n",
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(imputed_data)\n",
    "\n",
    "# 创建PCA模型并进行拟合\n",
    "pca = PCA(n_components=2)  # 选择要保留的主成分数量\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "# 将主成分转换为DataFrame\n",
    "principal_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "\n",
    "# 可视化主成分分析结果\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(principal_df['PC1'], principal_df['PC2'])\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('Principal Component Analysis')\n",
    "plt.show()\n",
    "\n",
    "# 查看PCA模型的载荷矩阵\n",
    "loadings = pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2'], index=continuous_features)\n",
    "print(loadings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "70641686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vehicle_id</th>\n",
       "      <th>position_x</th>\n",
       "      <th>position_y</th>\n",
       "      <th>position_z</th>\n",
       "      <th>heading</th>\n",
       "      <th>speed</th>\n",
       "      <th>speed_command</th>\n",
       "      <th>mileage_distance</th>\n",
       "      <th>power_on_time</th>\n",
       "      <th>soc</th>\n",
       "      <th>fuel_level</th>\n",
       "      <th>chassis_mode</th>\n",
       "      <th>estop</th>\n",
       "      <th>task_state_running</th>\n",
       "      <th>task_state_estop</th>\n",
       "      <th>task_state_lock</th>\n",
       "      <th>task_stage</th>\n",
       "      <th>current_task</th>\n",
       "      <th>error_code</th>\n",
       "      <th>target_location</th>\n",
       "      <th>vesselVisitID</th>\n",
       "      <th>mission_type</th>\n",
       "      <th>container1_type</th>\n",
       "      <th>container2_type</th>\n",
       "      <th>vehicle_mode</th>\n",
       "      <th>missionID</th>\n",
       "      <th>port</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>avg_speed_5min</th>\n",
       "      <th>needs_charging</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A022</td>\n",
       "      <td>154.233978</td>\n",
       "      <td>-218.729309</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11339</td>\n",
       "      <td>30930888</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.444533e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030909</td>\n",
       "      <td>0.018724</td>\n",
       "      <td>0.015729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>365fbfb4-9ecc-11ed-ac70-0242ac110030</td>\n",
       "      <td>tianhaiheda</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A022</td>\n",
       "      <td>154.221878</td>\n",
       "      <td>-218.728104</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11339</td>\n",
       "      <td>30936910</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.444533e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030909</td>\n",
       "      <td>0.018724</td>\n",
       "      <td>0.015729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>365fbfb4-9ecc-11ed-ac70-0242ac110030</td>\n",
       "      <td>tianhaiheda</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A028</td>\n",
       "      <td>-173.617371</td>\n",
       "      <td>13.083996</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>3.529296</td>\n",
       "      <td>3.234201</td>\n",
       "      <td>11419</td>\n",
       "      <td>31048816</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.444533e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.144994e-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6c9eb5ba-9fdc-11ed-8e85-0242ac11003c</td>\n",
       "      <td>changjinxiantai</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>5.109102</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A022</td>\n",
       "      <td>-269.873993</td>\n",
       "      <td>-97.462982</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.004478</td>\n",
       "      <td>1.236010</td>\n",
       "      <td>1.225601</td>\n",
       "      <td>11332</td>\n",
       "      <td>30914202</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.444533e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.953511e-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4d15657e-9ea4-11ed-8856-0242ac110030</td>\n",
       "      <td>tianhaiheda</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.962830</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A022</td>\n",
       "      <td>-248.316193</td>\n",
       "      <td>2.064377</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.003248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11334</td>\n",
       "      <td>30915288</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.799157e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.307926e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>af0ad1c2-9ea6-11ed-8e1d-0242ac110030</td>\n",
       "      <td>tianhaiheda</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  vehicle_id  position_x  position_y  position_z   heading     speed  \\\n",
       "0       A022  154.233978 -218.729309           0  0.000933  0.000000   \n",
       "1       A022  154.221878 -218.728104           0  0.000567  0.000000   \n",
       "2       A028 -173.617371   13.083996           0 -0.000052  3.529296   \n",
       "3       A022 -269.873993  -97.462982           0 -0.004478  1.236010   \n",
       "4       A022 -248.316193    2.064377           0 -0.003248  0.000000   \n",
       "\n",
       "   speed_command  mileage_distance  power_on_time  soc  fuel_level  \\\n",
       "0       0.000000             11339       30930888   72           0   \n",
       "1       0.000000             11339       30936910   72           0   \n",
       "2       3.234201             11419       31048816   59           0   \n",
       "3       1.225601             11332       30914202   79           0   \n",
       "4       0.000000             11334       30915288   78           0   \n",
       "\n",
       "   chassis_mode  estop  task_state_running  task_state_estop  task_state_lock  \\\n",
       "0             1      0                   0                 0                0   \n",
       "1             1      0                   0                 0                0   \n",
       "2             1      0                   1                 0                0   \n",
       "3             1      0                   1                 0                0   \n",
       "4             1      0                   0                 0                0   \n",
       "\n",
       "     task_stage  current_task  error_code  target_location  vesselVisitID  \\\n",
       "0  2.444533e-02           0.0         0.0     0.000000e+00            0.0   \n",
       "1  2.444533e-02           0.0         0.0     0.000000e+00            0.0   \n",
       "2  2.444533e-02           0.0         0.0     2.144994e-06            0.0   \n",
       "3  2.444533e-02           0.0         0.0     7.953511e-10            0.0   \n",
       "4  4.799157e-08           0.0         0.0     1.307926e-08            0.0   \n",
       "\n",
       "   mission_type  container1_type  container2_type  vehicle_mode  \\\n",
       "0      0.030909         0.018724         0.015729           0.0   \n",
       "1      0.030909         0.018724         0.015729           0.0   \n",
       "2      0.000000         0.000000         0.015729           0.0   \n",
       "3      0.000000         0.000000         0.015729           0.0   \n",
       "4      0.000000         0.000000         0.015729           0.0   \n",
       "\n",
       "                              missionID             port  hour  day_of_week  \\\n",
       "0  365fbfb4-9ecc-11ed-ac70-0242ac110030      tianhaiheda    13            5   \n",
       "1  365fbfb4-9ecc-11ed-ac70-0242ac110030      tianhaiheda    15            5   \n",
       "2  6c9eb5ba-9fdc-11ed-8e85-0242ac11003c  changjinxiantai    21            6   \n",
       "3  4d15657e-9ea4-11ed-8856-0242ac110030      tianhaiheda     8            5   \n",
       "4  af0ad1c2-9ea6-11ed-8e1d-0242ac110030      tianhaiheda     9            5   \n",
       "\n",
       "   avg_speed_5min  needs_charging  \n",
       "0        0.000000               0  \n",
       "1        0.000000               0  \n",
       "2        5.109102               0  \n",
       "3        0.962830               0  \n",
       "4        0.000000               0  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d26e2e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vehicle_id             object\n",
      "position_x            float64\n",
      "position_y            float64\n",
      "position_z              int64\n",
      "heading               float64\n",
      "speed                 float64\n",
      "speed_command         float64\n",
      "mileage_distance        int64\n",
      "power_on_time           int64\n",
      "soc                     int64\n",
      "fuel_level              int64\n",
      "chassis_mode            int64\n",
      "estop                   int64\n",
      "task_state_running      int64\n",
      "task_state_estop        int64\n",
      "task_state_lock         int64\n",
      "task_stage            float64\n",
      "current_task          float64\n",
      "error_code            float64\n",
      "target_location       float64\n",
      "vesselVisitID         float64\n",
      "mission_type          float64\n",
      "container1_type       float64\n",
      "container2_type       float64\n",
      "vehicle_mode          float64\n",
      "missionID              object\n",
      "port                   object\n",
      "hour                    int64\n",
      "day_of_week             int64\n",
      "avg_speed_5min        float64\n",
      "needs_charging          int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def make_preprocessing(filter_data=None):\n",
    "    \"\"\"\n",
    "    预处理给定的 CSV 数据。\n",
    "\n",
    "    参数:\n",
    "        filter_data (str, 可选): 用于指定要过滤的车辆ID。如果没有提供该参数，则处理所有车辆的数据。\n",
    "\n",
    "    返回:\n",
    "        X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scaler, y_scaler\n",
    "    \"\"\"\n",
    "    # 读取数据\n",
    "    df = read_data_yl(args.data_path, filter_data=filter_data)\n",
    "    \n",
    "    # 指定特征列和目标列\n",
    "    # feature_cols = [\n",
    "    #     'position_x', 'position_y', 'position_z', 'heading', 'speed', 'speed_command',\n",
    "    #     'mileage_distance', 'power_on_time', 'soc', 'fuel_level', 'chassis_mode', \n",
    "    #     'estop', 'task_state_running', 'task_state_estop', 'task_state_lock', \n",
    "    #     'task_stage', 'current_task', 'error_code', 'target_location', \n",
    "    #     'vesselVisitID', 'mission_type', 'container1_type', 'container2_type', \n",
    "    #     'vehicle_mode', 'port','hour', 'day_of_week', 'avg_speed_5min'\n",
    "    # ]\n",
    "    \n",
    "    # # 将'needs_charging'设为目标变量\n",
    "    # target_col = 'needs_charging'\n",
    "    \n",
    "    # # 分离特征和目标变量\n",
    "    # X = df[feature_cols]\n",
    "    # y = df[target_col]\n",
    "    \n",
    "    # # 这里假设 'exogenous_data' 是某些特定的列\n",
    "    # # 例如: 车辆当前任务、任务类型等\n",
    "    # exogenous_data_cols = ['current_task', 'mission_type']\n",
    "    # exogenous_data = df[exogenous_data_cols]\n",
    "    \n",
    "    # # 检查每个特征列是否包含非数值型数据\n",
    "    # for col in X.columns:\n",
    "    #     if not pd.api.types.is_numeric_dtype(X[col]):\n",
    "    #         raise ValueError(f\"特征列 '{col}' 包含非数值型数据，无法进行标准化。\")\n",
    "    \n",
    "    # # 将数据分为训练集和验证集\n",
    "    X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val = train_test_split(\n",
    "    #     X, y, exogenous_data, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # # 初始化缩放器\n",
    "    # x_scaler = StandardScaler()\n",
    "    # y_scaler = StandardScaler()\n",
    "    # # 拟合并转换训练数据\n",
    "    # X_train_scaled = x_scaler.fit_transform(X_train)\n",
    "    # X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "\n",
    "    # # 转换验证数据\n",
    "    # X_val_scaled = x_scaler.transform(X_val)\n",
    "    # X_val = pd.DataFrame(X_val_scaled, columns=X_val.columns)\n",
    "\n",
    "    \n",
    "    # # 拟合并转换训练数据\n",
    "    # X_train = x_scaler.fit_transform(X_train)\n",
    "    # y_train = y_scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "    \n",
    "    # # 转换验证数据\n",
    "    # X_val = x_scaler.transform(X_val)\n",
    "    # y_val = y_scaler.transform(y_val.values.reshape(-1, 1))\n",
    "    \n",
    "    # return X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scaler, y_scaler\n",
    "    return df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66497131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例用法\n",
    "X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scaler, y_scaler = make_preprocessing(filter_data='tianhaiheda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5148a65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   position_x  position_y  position_z   heading     speed  speed_command  \\\n",
      "0    0.777461   -1.053085         0.0 -0.664916 -0.275249      -0.273404   \n",
      "1   -0.548309    0.811175         0.0  0.867434 -0.275249      -0.273404   \n",
      "2    2.140261    0.105426         0.0  0.893322  4.026245       4.171201   \n",
      "3    0.777376   -1.053034         0.0 -0.664030 -0.275249      -0.273404   \n",
      "4    0.777458   -1.053079         0.0 -0.664220 -0.275249      -0.273404   \n",
      "\n",
      "   mileage_distance  power_on_time       soc  fuel_level  chassis_mode  \\\n",
      "0          0.984277       0.521547 -0.943597         0.0     -0.299467   \n",
      "1         -0.561244      -0.226106  0.658042         0.0     -0.299467   \n",
      "2         -0.174864       0.150822  0.017386         0.0     -0.299467   \n",
      "3          0.984277       1.332137 -0.943597         0.0     -0.299467   \n",
      "4          0.984277       0.241321 -0.943597         0.0     -0.299467   \n",
      "\n",
      "      estop  task_state_running  task_state_estop  task_state_lock  \\\n",
      "0 -0.760983           -0.329172               0.0        -0.021986   \n",
      "1  1.314090           -0.329172               0.0        -0.021986   \n",
      "2 -0.760983            3.037926               0.0        -0.021986   \n",
      "3 -0.760983           -0.329172               0.0        -0.021986   \n",
      "4 -0.760983           -0.329172               0.0        -0.021986   \n",
      "\n",
      "   task_stage  current_task  error_code  target_location  vesselVisitID  \\\n",
      "0    0.841527     -0.033596   -0.027313        -0.045767      -0.177670   \n",
      "1   -1.189438     -0.033596   -0.027313        -0.045767      -0.177670   \n",
      "2    0.841527     -0.033596   -0.027313        -0.039255       5.628406   \n",
      "3    0.841527     -0.033596   -0.027313        -0.045767      -0.177670   \n",
      "4    0.841527     -0.033596   -0.027313        -0.045767      -0.177670   \n",
      "\n",
      "   mission_type  container1_type  container2_type  vehicle_mode      hour  \\\n",
      "0      1.025972         0.379598              0.0     -0.640050  0.614565   \n",
      "1     -0.975702         0.379598              0.0      1.562593 -0.173912   \n",
      "2      1.025972         0.379598              0.0     -0.640050  0.220327   \n",
      "3      1.025972         0.379598              0.0     -0.640050  1.403043   \n",
      "4      1.025972         0.379598              0.0     -0.640050  0.220327   \n",
      "\n",
      "   day_of_week  avg_speed_5min  \n",
      "0          0.0       -0.283053  \n",
      "1          0.0       -0.283053  \n",
      "2          0.0        5.186655  \n",
      "3          0.0       -0.283053  \n",
      "4          0.0       -0.283053  \n"
     ]
    }
   ],
   "source": [
    "print(X_train[:5])  # 查看前5行数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "367c2b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(StandardScaler(), StandardScaler())"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaler, y_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488a9c30",
   "metadata": {},
   "source": [
    "### Postprocessing Stage\n",
    "\n",
    "In this stage we transform data in a way that can be fed into ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b19e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def make_postprocessing(X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scaler, y_scaler, args):\n",
    "    \"\"\"\n",
    "    对数据进行后处理，使其准备好输入机器学习算法。\n",
    "    \n",
    "    参数:\n",
    "        X_train, X_val: 训练集和验证集的特征数据。\n",
    "        y_train, y_val: 训练集和验证集的目标数据。\n",
    "        exogenous_data_train, exogenous_data_val: 训练集和验证集的外生数据。\n",
    "        x_scaler, y_scaler: 特征数据和目标数据的缩放器。\n",
    "        args: 包含必要信息的其他参数。\n",
    "\n",
    "    返回:\n",
    "        X_train, X_val, y_train, y_val, area_X_train, area_X_val, area_y_train, area_y_val, exogenous_data_train, exogenous_data_val\n",
    "    \"\"\"\n",
    "\n",
    "    # 检查训练数据集中是否有多个不同的区域。\n",
    "    if X_train[args.identifier].nunique() != 1:\n",
    "        area_X_train, area_X_val, area_y_train, area_y_val = get_data_by_area_yl(X_train, X_val,\n",
    "                                                                              y_train, y_val, \n",
    "                                                                              identifier=args.identifier)\n",
    "    else:\n",
    "        area_X_train, area_X_val, area_y_train, area_y_val = None, None, None, None\n",
    "\n",
    "    # 获取每个区域的外生数据。\n",
    "    if exogenous_data_train is not None:\n",
    "        exogenous_data_train, exogenous_data_val = get_exogenous_data_by_area_yl(exogenous_data_train,\n",
    "                                                                              exogenous_data_val)\n",
    "\n",
    "    # 将区域分割的数据转换为 NumPy 数组并移除标识符列。\n",
    "    if area_X_train is not None:\n",
    "        for area in area_X_train:\n",
    "            tmp_X_train, tmp_y_train, tmp_X_val, tmp_y_val = remove_identifiers_yl(\n",
    "                area_X_train[area], area_y_train[area], area_X_val[area], area_y_val[area])\n",
    "            tmp_X_train, tmp_y_train = tmp_X_train.to_numpy(), tmp_y_train.to_numpy()\n",
    "            tmp_X_val, tmp_y_val = tmp_X_val.to_numpy(), tmp_y_val.to_numpy()\n",
    "            area_X_train[area] = tmp_X_train\n",
    "            area_X_val[area] = tmp_X_val\n",
    "            area_y_train[area] = tmp_y_train\n",
    "            area_y_val[area] = tmp_y_val\n",
    "    \n",
    "    # 将外生数据转换为 NumPy 数组。\n",
    "    if exogenous_data_train is not None:\n",
    "        for area in exogenous_data_train:\n",
    "            exogenous_data_train[area] = exogenous_data_train[area].to_numpy()\n",
    "            exogenous_data_val[area] = exogenous_data_val[area].to_numpy()\n",
    "\n",
    "    # 从特征和目标数据中移除标识符列。\n",
    "    X_train, y_train, X_val, y_val = remove_identifiers_yl(X_train, y_train, X_val, y_val)\n",
    "    assert len(X_train.columns) == len(X_val.columns)\n",
    "\n",
    "    # 计算特征数量，考虑滞后期数量。\n",
    "    num_features = len(X_train.columns) // args.num_lags\n",
    "\n",
    "    # 将特征数据转换为时间序列表示。\n",
    "    X_train = to_timeseries_rep(X_train.to_numpy(), num_lags=args.num_lags, num_features=num_features)\n",
    "    X_val = to_timeseries_rep(X_val.to_numpy(), num_lags=args.num_lags, num_features=num_features)\n",
    "\n",
    "    if area_X_train is not None:\n",
    "        area_X_train = to_timeseries_rep(area_X_train, num_lags=args.num_lags, num_features=num_features)\n",
    "        area_X_val = to_timeseries_rep(area_X_val, num_lags=args.num_lags, num_features=num_features)\n",
    "\n",
    "    # 将目标数据转换为 NumPy 数组。\n",
    "    y_train, y_val = y_train.to_numpy(), y_val.to_numpy()\n",
    "\n",
    "    # 如果是集中学习，则将所有区域的外生数据合并。\n",
    "    if not args.filter_bs and exogenous_data_train is not None:\n",
    "        exogenous_data_train_combined, exogenous_data_val_combined = [], []\n",
    "        for area in exogenous_data_train:\n",
    "            exogenous_data_train_combined.extend(exogenous_data_train[area])\n",
    "            exogenous_data_val_combined.extend(exogenous_data_val[area])\n",
    "        exogenous_data_train_combined = np.stack(exogenous_data_train_combined)\n",
    "        exogenous_data_val_combined = np.stack(exogenous_data_val_combined)\n",
    "        exogenous_data_train[\"all\"] = exogenous_data_train_combined\n",
    "        exogenous_data_val[\"all\"] = exogenous_data_val_combined\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val, area_X_train, area_X_val, area_y_train, area_y_val, exogenous_data_train, exogenous_data_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c417082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_postprocessing(X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scaler, y_scaler, args):\n",
    "    #X_train, X_val: 训练集和验证集的特征数据。\n",
    "    #y_train, y_val: 训练集和验证集的目标数据。\n",
    "    #exogenous_data_train, exogenous_data_val: 训练集和验证集的外生数据。\n",
    "    #x_scaler, y_scaler: 特征数据和目标数据的缩放器。\n",
    "    \"\"\"Make data ready to be fed into ml algorithms\"\"\"\n",
    "    # if there are more than one specified areas, get the data per area \n",
    "    #检查训练数据集中是否有多个不同的区域（由 args.identifier 标识）。如果有多个区域，则调用 get_data_by_area 函数将数据按区域分割。\n",
    "    if X_train[args.identifier].nunique() != 1:\n",
    "        area_X_train, area_X_val, area_y_train, area_y_val = get_data_by_area(X_train, X_val,\n",
    "                                                                              y_train, y_val, \n",
    "                                                                              identifier=args.identifier)\n",
    "    else:\n",
    "        area_X_train, area_X_val, area_y_train, area_y_val = None, None, None, None\n",
    "\n",
    "    # Get the exogenous data per area. 获取每个区域的外生数据 如果存在外生数据，则将其按区域分割。\n",
    "    if exogenous_data_train is not None:\n",
    "        exogenous_data_train, exogenous_data_val = get_exogenous_data_by_area(exogenous_data_train,\n",
    "                                                                              exogenous_data_val)\n",
    "    # transform to np 将区域划分的数据转换为 NumPy 数组，并移除标识符列（如区域名称）。\n",
    "    if area_X_train is not None:\n",
    "        for area in area_X_train:\n",
    "            tmp_X_train, tmp_y_train, tmp_X_val, tmp_y_val = remove_identifiers(\n",
    "                area_X_train[area], area_y_train[area], area_X_val[area], area_y_val[area])\n",
    "            tmp_X_train, tmp_y_train = tmp_X_train.to_numpy(), tmp_y_train.to_numpy()\n",
    "            tmp_X_val, tmp_y_val = tmp_X_val.to_numpy(), tmp_y_val.to_numpy()\n",
    "            area_X_train[area] = tmp_X_train\n",
    "            area_X_val[area] = tmp_X_val\n",
    "            area_y_train[area] = tmp_y_train\n",
    "            area_y_val[area] = tmp_y_val\n",
    "    \n",
    "    if exogenous_data_train is not None:\n",
    "        for area in exogenous_data_train:\n",
    "            exogenous_data_train[area] = exogenous_data_train[area].to_numpy()\n",
    "            exogenous_data_val[area] = exogenous_data_val[area].to_numpy()\n",
    "    \n",
    "    # remove identifiers from features, targets 对整体数据集，移除标识符列，并确保训练集和验证集的特征列数相同。\n",
    "    X_train, y_train, X_val, y_val = remove_identifiers(X_train, y_train, X_val, y_val)\n",
    "    assert len(X_train.columns) == len(X_val.columns)\n",
    "    #计算特征数量，这里每个特征都有多个滞后期，所以除以滞后期的数量\n",
    "    num_features = len(X_train.columns) // args.num_lags\n",
    "    \n",
    "    # to timeseries representation 将特征数据转换为时间序列表示，即构建时间滞后的特征集\n",
    "    X_train = to_timeseries_rep(X_train.to_numpy(), num_lags=args.num_lags,\n",
    "                                            num_features=num_features)\n",
    "    X_val = to_timeseries_rep(X_val.to_numpy(), num_lags=args.num_lags,\n",
    "                                          num_features=num_features)\n",
    "    \n",
    "    if area_X_train is not None:\n",
    "        area_X_train = to_timeseries_rep(area_X_train, num_lags=args.num_lags,\n",
    "                                                     num_features=num_features)\n",
    "        area_X_val = to_timeseries_rep(area_X_val, num_lags=args.num_lags,\n",
    "                                                   num_features=num_features)\n",
    "    \n",
    "    # transform targets to numpy 将目标数据转换为 NumPy 数组\n",
    "    y_train, y_val = y_train.to_numpy(), y_val.to_numpy()\n",
    "    \n",
    "    # centralized (all) learning specific 在集中学习的情况下，将所有区域的外生数据合并成一个数据集\n",
    "    if not args.filter_bs and exogenous_data_train is not None:\n",
    "        exogenous_data_train_combined, exogenous_data_val_combined = [], []\n",
    "        for area in exogenous_data_train:\n",
    "            exogenous_data_train_combined.extend(exogenous_data_train[area])\n",
    "            exogenous_data_val_combined.extend(exogenous_data_val[area])\n",
    "        exogenous_data_train_combined = np.stack(exogenous_data_train_combined)\n",
    "        exogenous_data_val_combined = np.stack(exogenous_data_val_combined)\n",
    "        exogenous_data_train[\"all\"] = exogenous_data_train_combined\n",
    "        exogenous_data_val[\"all\"] = exogenous_data_val_combined\n",
    "    return X_train, X_val, y_train, y_val, area_X_train, area_X_val, area_y_train, area_y_val, exogenous_data_train, exogenous_data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c59dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val, area_X_train, area_X_val, area_y_train, area_y_val, exogenous_data_train, exogenous_data_val = make_postprocessing(X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scaler, y_scaler, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9171667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60d4f55",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2949d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7bb251",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train), len(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a83357",
   "metadata": {},
   "source": [
    "### Define the input dimensions for the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1486173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_dims(X_train, exogenous_data_train): \n",
    "    #计算模型输入的维度，X_train: 训练集的主输入数据（通常是一个多维数组）；exogenous_data_train: 训练集的外生数据（可以是 None 或包含外生特征的字典）。\n",
    "    if args.model_name == \"mlp\":\n",
    "        input_dim = X_train.shape[1] * X_train.shape[2] #如果模型是多层感知机（MLP），则将输入的所有特征展平成一个一维向量，因此输入维度是 X_train 的第二维度和第三维度的乘积（即 X_train.shape[1] * X_train.shape[2]）\n",
    "    else:\n",
    "        input_dim = X_train.shape[2] #对于其他模型（如卷积神经网络或循环神经网络），输入维度保持为 X_train 的第三维度（即 X_train.shape[2]）\n",
    "    #计算外生数据的维度\n",
    "    if exogenous_data_train is not None: #如果 exogenous_data_train 不为 None\n",
    "        if len(exogenous_data_train) == 1: #如果外生数据中只有一个区域\n",
    "            cid = next(iter(exogenous_data_train.keys()))\n",
    "            exogenous_dim = exogenous_data_train[cid].shape[1] #从字典中获取该区域的维度。\n",
    "        else:\n",
    "            exogenous_dim = exogenous_data_train[\"all\"].shape[1] #如果外生数据中有多个区域，则使用键为 \"all\" 的区域的维度\n",
    "    else:\n",
    "        exogenous_dim = 0 #如果 exogenous_data_train 为 None，则外生数据的维度为 0\n",
    "    \n",
    "    return input_dim, exogenous_dim #input_dim: 主输入数据的维度；exogenous_dim: 外生数据的维度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6783ff",
   "metadata": {},
   "source": [
    "### Initialize the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7044d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model: str,\n",
    "              input_dim: int,\n",
    "              out_dim: int,\n",
    "              lags: int = 10,\n",
    "              exogenous_dim: int = 0,\n",
    "              seed=0):\n",
    "    if model == \"mlp\":\n",
    "        model = MLP(input_dim=input_dim, layer_units=[256, 128, 64], num_outputs=out_dim)\n",
    "    elif model == \"rnn\":\n",
    "        model = RNN(input_dim=input_dim, rnn_hidden_size=128, num_rnn_layers=1, rnn_dropout=0.0,\n",
    "                    layer_units=[128], num_outputs=out_dim, matrix_rep=True, exogenous_dim=exogenous_dim)\n",
    "    elif model == \"lstm\":\n",
    "        model = LSTM(input_dim=input_dim, lstm_hidden_size=128, num_lstm_layers=1, lstm_dropout=0.0,\n",
    "                     layer_units=[128], num_outputs=out_dim, matrix_rep=True, exogenous_dim=exogenous_dim)\n",
    "    elif model == \"gru\":\n",
    "        model = GRU(input_dim=input_dim, gru_hidden_size=128, num_gru_layers=1, gru_dropout=0.0,\n",
    "                    layer_units=[128], num_outputs=out_dim, matrix_rep=True, exogenous_dim=exogenous_dim)\n",
    "    elif model == \"cnn\":\n",
    "        model = CNN(num_features=input_dim, lags=lags, exogenous_dim=exogenous_dim, out_dim=out_dim)\n",
    "    elif model == \"da_encoder_decoder\":\n",
    "        model = DualAttentionAutoEncoder(input_dim=input_dim, architecture=\"lstm\", matrix_rep=True)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Specified model is not implemented. Plese define your own model or choose one from ['mlp', 'rnn', 'lstm', 'gru', 'cnn', 'da_encoder_decoder']\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ed1dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "args.model_name = \"mlp\"\n",
    "\n",
    "input_dim, exogenous_dim = get_input_dims(X_train, exogenous_data_train)\n",
    "\n",
    "print(input_dim, exogenous_dim)\n",
    "\n",
    "model = get_model(model=args.model_name,\n",
    "                  input_dim=input_dim,\n",
    "                  out_dim=y_train.shape[1],\n",
    "                  lags=args.num_lags,\n",
    "                  exogenous_dim=exogenous_dim,\n",
    "                  seed=args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f285c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2473d72",
   "metadata": {},
   "source": [
    "### The fit function used to train the model specified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b11fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, X_train, y_train, X_val, y_val, #model: 需要训练的模型；X_train, y_train: 训练数据和标签；X_val, y_val: 验证数据和标签\n",
    "        exogenous_data_train=None, exogenous_data_val=None, #exogenous_data_train, exogenous_data_val: 训练和验证数据的外生特征\n",
    "        idxs=[8, 3, 1, 10, 9], # the indices of our targets in X ；idxs: 目标变量在输入数据中的索引\n",
    "        log_per=1): #log_per: 记录训练日志的频率\n",
    "    \n",
    "    # get exogenous data (if any)\n",
    "    if exogenous_data_train is not None and len(exogenous_data_train) > 1:\n",
    "        exogenous_data_train = exogenous_data_train[\"all\"]\n",
    "        exogenous_data_val = exogenous_data_val[\"all\"]\n",
    "    elif exogenous_data_train is not None and len(exogenous_data_train) == 1:\n",
    "        cid = next(iter(exogenous_data_train.keys()))\n",
    "        exogenous_data_train = exogenous_data_train[cid]\n",
    "        exogenous_data_val = exogenous_data_val[cid]\n",
    "    else:\n",
    "        exogenous_data_train = None\n",
    "        exogenous_data_val = None\n",
    "    num_features = len(X_train[0][0]) #计算特征数量，X_train 是一个 3D 数组，形状为 (num_samples, num_lags, num_features)\n",
    "    \n",
    "    # to torch loader\n",
    "    train_loader = to_torch_dataset(X_train, y_train,\n",
    "                                    num_lags=args.num_lags,\n",
    "                                    num_features=num_features,\n",
    "                                    exogenous_data=exogenous_data_train,\n",
    "                                    indices=idxs,\n",
    "                                    batch_size=args.batch_size, \n",
    "                                    shuffle=False)\n",
    "    val_loader = to_torch_dataset(X_val, y_val, \n",
    "                                  num_lags=args.num_lags,\n",
    "                                  num_features=num_features,\n",
    "                                  exogenous_data=exogenous_data_val,\n",
    "                                  indices=idxs,\n",
    "                                  batch_size=args.batch_size,\n",
    "                                  shuffle=False)\n",
    "    \n",
    "    # train the model\n",
    "    model = train(model, \n",
    "                  train_loader, val_loader,\n",
    "                  epochs=args.epochs,\n",
    "                  optimizer=args.optimizer, lr=args.lr,\n",
    "                  criterion=args.criterion,\n",
    "                  early_stopping=args.early_stopping,\n",
    "                  patience=args.patience,\n",
    "                  plot_history=args.plot_history, \n",
    "                  device=device, log_per=log_per)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681cc512",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = fit(model, X_train, y_train, X_val, y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "a39106e1a9d6d153b7400628e7589ff266b5caee5b0db427f0903be982155882"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
