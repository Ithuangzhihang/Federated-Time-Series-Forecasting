{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8da638ca",
   "metadata": {},
   "source": [
    "### In this notebook we perform individual training.\n",
    "In individual learning each base station has access only to it's private dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c3fe1f1-1806-43a8-a467-516338fea3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "# 解释：将当前目录的父目录加入到sys.path中，这样就可以在当前目录下导入父目录的模块\n",
    "parent = Path(os.path.abspath(\"\")).resolve().parents[0]\n",
    "if parent not in sys.path:\n",
    "    sys.path.insert(0, str(parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01e170e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15abc428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解释\n",
    "from ml.utils.data_utils_yl import read_data, generate_time_lags, time_to_feature, handle_nans, to_Xy, \\\n",
    "    to_torch_dataset, to_timeseries_rep, assign_statistics, \\\n",
    "    to_train_val, scale_features, get_data_by_area, remove_identifiers, get_exogenous_data_by_area, handle_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "350c9d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.utils.train_utils import train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4688fbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.models.mlp import MLP\n",
    "from ml.models.rnn import RNN\n",
    "from ml.models.lstm import LSTM\n",
    "from ml.models.gru import GRU\n",
    "from ml.models.cnn import CNN\n",
    "from ml.models.rnn_autoencoder import DualAttentionAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3db1750",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    #数据相关参数\n",
    "    data_path='../dataset/full_dataset.csv', # dataset  训练集路径\n",
    "    data_path_test=['../dataset/ElBorn_test.csv'], # test dataset  测试集路径\n",
    "    test_size=0.2, # validation size  验证集大小 数据划分为训练集和验证集时使用的比例,据的20%将用于验证，其余80%用于训练\n",
    "    targets=['rnti_count', 'rb_down', 'rb_up', 'down', 'up'], # the target columns 需要预测的列，指定模型的目标输出。 \n",
    "    #时间序列相关参数\n",
    "    num_lags=10, # the number of past observations to feed as input  在时间序列预测中，过去的观测值对未来的预测有重要影响。这个参数指定了使用多少个过去的时间步作为输入。\n",
    "\n",
    "    #数据处理相关参数\n",
    "    filter_bs=None, # whether to use a single bs for training. It will be changed dynamically 是否只使用单个基站进行训练，在这里设置为None，表示不使用这个功能。\n",
    "    identifier='District', # the column name that identifies a bs 标识基站的列名。\n",
    "\n",
    "    nan_constant=0, # the constant to transform nan values 替换NaN值的常数。 \n",
    "    x_scaler='minmax', # x_scaler 数据标准化的方法，使用最小-最大缩放（Min-Max Scaler）。\n",
    "    y_scaler='minmax', # y_scaler 数据标准化的方法，使用最小-最大缩放（Min-Max Scaler）。\n",
    "    outlier_detection=None, # whether to perform flooring and capping 是否进行异常值检测。在这里设置为None，表示不使用这个功能。\n",
    "\n",
    "    #模型训练相关参数\n",
    "    criterion='mse', # optimization criterion, mse or l1 损失函数，MSE（均方误差）用于衡量模型预测值与真实值之间的差异。\n",
    "    epochs=150, # the number of maximum epochs 训练的最大迭代次数（轮数）。\n",
    "    lr=0.001, # learning rate 学习率，控制模型权重更新的步长。\n",
    "    optimizer='adam', # the optimizer, it can be sgd or adam 优化器，Adam是一种自适应学习率优化算法，广泛用于深度学习。\n",
    "    batch_size=128, # the batch size to use 批量大小，指每次迭代中使用的样本数量。\n",
    "    early_stopping=True, # whether to use early stopping 早停机制，防止过拟合。当验证集性能不再提升时提前停止训练。\n",
    "    patience=50, # patience value for the early stopping parameter (if specified) 提前停止的耐心值，当验证集性能在连续50轮训练中没有提升时停止训练。\n",
    "    max_grad_norm=0.0, # whether to clip grad norm 最大梯度范数，用于梯度裁剪。如果为0.0，表示不进行梯度裁剪。\n",
    "    reg1=0.0, # l1 regularization L1正则化参数，防止过拟合。在这里设置为0.0，表示未使用L1正则化。\n",
    "    reg2=0.0, # l2 regularization L2正则化参数，防止过拟合。在这里设置为0.0，表示未使用L2正则化。\n",
    "    \n",
    "    plot_history=True, # plot loss history 是否绘制训练过程中的损失和评价指标曲线图。\n",
    "    #硬件和复现性相关参数\n",
    "    cuda=True, # whether to use gpu 是否使用GPU进行计算。如果为True，则使用CUDA加速。\n",
    "    \n",
    "    seed=0, # reproducibility 随机种子，用于确保结果的可重复性。\n",
    "    #其他参数\n",
    "    assign_stats=None, # whether to use statistics as exogenous data, [\"mean\", \"median\", \"std\", \"variance\", \"kurtosis\", \"skew\"] 是否使用统计数据作为外生数据，选项包括[\"mean\", \"median\", \"std\", \"variance\", \"kurtosis\", \"skew\"]。在这里设置为None，表示未使用这个功能。\n",
    "    use_time_features=False # whether to use datetime features 是否使用日期时间特征。如果为True，表示在特征中加入时间信息（如时间戳）。\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56a743e",
   "metadata": {},
   "source": [
    "> You can define the base station to perform train on the filter_bs parameter and use it in block 12 or you can define the base station to block 12 explicitly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "763c39ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script arguments: Namespace(assign_stats=None, batch_size=128, criterion='mse', cuda=True, data_path='../dataset/full_dataset.csv', data_path_test=['../dataset/ElBorn_test.csv'], early_stopping=True, epochs=150, filter_bs=None, identifier='District', lr=0.001, max_grad_norm=0.0, nan_constant=0, num_lags=10, optimizer='adam', outlier_detection=None, patience=50, plot_history=True, reg1=0.0, reg2=0.0, seed=0, targets=['rnti_count', 'rb_down', 'rb_up', 'down', 'up'], test_size=0.2, use_time_features=False, x_scaler='minmax', y_scaler='minmax')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Script arguments: {args}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fa39785-522b-4467-b525-d353856bb582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script arguments: Namespace(assign_stats=None, batch_size=128, criterion='mse', cuda=True, data_path='../dataset/full_dataset.csv', data_path_test=['../dataset/ElBorn_test.csv'], early_stopping=True, epochs=150, filter_bs=None, identifier='District', lr=0.001, max_grad_norm=0.0, nan_constant=0, num_lags=10, optimizer='adam', outlier_detection=None, patience=50, plot_history=True, reg1=0.0, reg2=0.0, seed=0, targets=['rnti_count', 'rb_down', 'rb_up', 'down', 'up'], test_size=0.2, use_time_features=False, x_scaler='minmax', y_scaler='minmax')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Script arguments: {args}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da3431ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch_version 2.3.0\n",
      "args.cuda True\n",
      "torch.cuda.is_available() False\n",
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "import torch.version\n",
    "\n",
    "\n",
    "device = \"cuda\" if args.cuda and torch.cuda.is_available() else \"cpu\"\n",
    "print(\"torch_version\",torch.__version__)\n",
    "print(\"args.cuda\",args.cuda)\n",
    "print(\"torch.cuda.is_available()\",torch.cuda.is_available())\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06bb4aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection specification 异常值检测\n",
    "if args.outlier_detection is not None:\n",
    "    outlier_columns = ['rb_down', 'rb_up', 'down', 'up'] #需要进行异常值检测的列名\n",
    "    outlier_kwargs = {\"ElBorn\": (10, 90), \"LesCorts\": (10, 90), \"PobleSec\": (5, 95)} #每个键是一个区域的名称，每个值是一个元组，用于指定该区域的异常值检测参数。\"ElBorn\": (10, 90)：表示对于ElBorn区域，异常值检测的阈值是10和90。\n",
    "    args.outlier_columns = outlier_columns \n",
    "    args.outlier_kwargs = outlier_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ac1d35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all():\n",
    "    # ensure reproducibility 确保结果的可重复性\n",
    "    random.seed(args.seed) #设置Python标准库中的随机数生成器的种子为args.seed。这会影响使用random模块生成的所有随机数。\n",
    "    np.random.seed(args.seed) #设置NumPy库的随机数生成器的种子为args.seed。这会影响使用numpy.random模块生成的所有随机数。\n",
    "    torch.manual_seed(args.seed) #设置PyTorch库的随机数生成器的种子为args.seed。这会影响CPU上的所有PyTorch操作生成的随机数。\n",
    "    torch.cuda.manual_seed_all(args.seed) #设置所有CUDA设备（即GPU）的随机数生成器的种子为args.seed。这会影响在GPU上执行的所有PyTorch操作生成的随机数。\n",
    "    torch.backends.cudnn.deterministic = True #设置CuDNN后端为确定性模式，这意味着CuDNN将使用确定性的算法，从而确保相同的输入始终产生相同的输出。\n",
    "    torch.backends.cudnn.benchmark = False #禁用CuDNN的benchmark模式。启用benchmark模式可能会导致不同的计算选择不同的算法，从而产生不同的结果，因此禁用它可以确保结果的一致性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea3ddd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00543376",
   "metadata": {},
   "source": [
    "### The preprocessing pipeline performed here for the base station specified in filter_bs argument\n",
    "Preprocessing inlcudes:\n",
    "1. NaNs Handling NANs处理\n",
    "2. Outliers Handling 异常值处理\n",
    "3. Scaling Data 数据缩放\n",
    "4. Generating time lags 生成时间滞后\n",
    "5. Generating and importing exogenous data as features (time, statistics) (if applied) 生成和导入外生数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35bc6b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preprocessing(filter_bs=None): \n",
    "    #本函数接受一个可选参数‘filter_bs’,用于指定要过滤的基站。如果没有提供该参数，则处理所有基站的数据。\n",
    "    \"\"\"Preprocess a given .csv\"\"\"\n",
    "    # read data 使用read_data函数读取指定路径（args.data_path）的CSV数据，并根据filter_bs过滤数据。\n",
    "    df = read_data(args.data_path, filter_data=filter_bs)\n",
    "    # handle nans 使用handle_nans函数处理数据中的缺失值。缺失值将被替换为args.nan_constant指定的常数。\n",
    "    df = handle_nans(train_data=df, constant=args.nan_constant,\n",
    "                     identifier=args.identifier)\n",
    "    # split to train/validation 将数据集划分为训练集和验证集。具体的划分比例由预先定义的参数决定。\n",
    "    train_data, val_data = to_train_val(df)\n",
    "    \n",
    "    # handle outliers (if specified) 如果启用了异常值检测（args.outlier_detection不为None），则使用handle_outliers函数处理训练数据中的异常值。\n",
    "    if args.outlier_detection is not None:\n",
    "        train_data = handle_outliers(df=train_data, columns=args.outlier_columns,\n",
    "                                     identifier=args.identifier, kwargs=args.outlier_kwargs)\n",
    "    \n",
    "    # get X and y 将训练集和验证集中的特征（X）和目标变量（y）分离出来。\n",
    "    X_train, X_val, y_train, y_val = to_Xy(train_data=train_data, val_data=val_data,\n",
    "                                          targets=args.targets)\n",
    "    # 对特征数据和目标变量分别进行缩放（标准化或归一化），以确保它们在相似的尺度上。缩放方法由args.x_scaler和args.y_scaler指定。\n",
    "    # scale X\n",
    "    X_train, X_val, x_scaler = scale_features(train_data=X_train, val_data=X_val,\n",
    "                                             scaler=args.x_scaler, identifier=args.identifier)\n",
    "    # scale y\n",
    "    y_train, y_val, y_scaler = scale_features(train_data=y_train, val_data=y_val,\n",
    "                                             scaler=args.y_scaler, identifier=args.identifier)\n",
    "    \n",
    "    # generate time lags 生成时间滞后特征，即在时间序列数据中，使用过去若干时间步的数据作为当前时间步的输入特征。\n",
    "    X_train = generate_time_lags(X_train, args.num_lags)\n",
    "    X_val = generate_time_lags(X_val, args.num_lags)\n",
    "    y_train = generate_time_lags(y_train, args.num_lags, is_y=True)\n",
    "    y_val = generate_time_lags(y_val, args.num_lags, is_y=True)\n",
    "    \n",
    "    #生成并导入时间特征（如时间戳、日期等）和统计特征（如均值、方差等），作为外生数据特征。\n",
    "    # get datetime features as exogenous data\n",
    "    date_time_df_train = time_to_feature(\n",
    "        X_train, args.use_time_features, identifier=args.identifier\n",
    "    )\n",
    "    date_time_df_val = time_to_feature(\n",
    "        X_val, args.use_time_features, identifier=args.identifier\n",
    "    )\n",
    "    \n",
    "    # get statistics as exogenous data\n",
    "    stats_df_train = assign_statistics(X_train, args.assign_stats, args.num_lags,\n",
    "                                       targets=args.targets, identifier=args.identifier)\n",
    "    stats_df_val = assign_statistics(X_val, args.assign_stats, args.num_lags, \n",
    "                                       targets=args.targets, identifier=args.identifier)\n",
    "    \n",
    "    #合并生成的外生特征（时间特征和统计特征），并去除重复的列。如果没有外生特征，则设为None。\n",
    "    #exogenous_data_train 和 exogenous_data_val如果不为 None，这些是训练集和验证集的外生特征数据。\n",
    "    # concat the exogenous features (if any) to a single dataframe\n",
    "    if date_time_df_train is not None or stats_df_train is not None:\n",
    "        exogenous_data_train = pd.concat([date_time_df_train, stats_df_train], axis=1)\n",
    "        # remove duplicate columns (if any)\n",
    "        exogenous_data_train = exogenous_data_train.loc[:, ~exogenous_data_train.columns.duplicated()].copy()\n",
    "        assert len(exogenous_data_train) == len(X_train) == len(y_train)\n",
    "    else:\n",
    "        exogenous_data_train = None\n",
    "    if date_time_df_val is not None or stats_df_val is not None:\n",
    "        exogenous_data_val = pd.concat([date_time_df_val, stats_df_val], axis=1)\n",
    "        exogenous_data_val = exogenous_data_val.loc[:, ~exogenous_data_val.columns.duplicated()].copy()\n",
    "        assert len(exogenous_data_val) == len(X_val) == len(y_val)\n",
    "    else:\n",
    "        exogenous_data_val = None\n",
    "        \n",
    "    return X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scaler, y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66fc93eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO logger 2024-06-04 00:00:12,209 | data_utils.py:28 | Reading LesCorts's data...\n",
      "INFO logger 2024-06-04 00:00:12,229 | data_utils.py:565 | \tTotal number of samples:  6892\n",
      "INFO logger 2024-06-04 00:00:12,229 | data_utils.py:566 | \tNumber of samples for training: 5514\n",
      "INFO logger 2024-06-04 00:00:12,229 | data_utils.py:567 | \tNumber of samples for validation:  1378\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/yanglin/Federated-Time-Series-Forecasting/notebooks/02.Individual_Training.ipynb 单元格 17\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B%E5%AE%9E%E9%AA%8C%E6%9C%BA%E5%99%A8/home/yanglin/Federated-Time-Series-Forecasting/notebooks/02.Individual_Training.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# here exogenous_data_train and val are None.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B%E5%AE%9E%E9%AA%8C%E6%9C%BA%E5%99%A8/home/yanglin/Federated-Time-Series-Forecasting/notebooks/02.Individual_Training.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scaler, y_scaler \u001b[39m=\u001b[39m make_preprocessing(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B%E5%AE%9E%E9%AA%8C%E6%9C%BA%E5%99%A8/home/yanglin/Federated-Time-Series-Forecasting/notebooks/02.Individual_Training.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     filter_bs\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mLesCorts\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B%E5%AE%9E%E9%AA%8C%E6%9C%BA%E5%99%A8/home/yanglin/Federated-Time-Series-Forecasting/notebooks/02.Individual_Training.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m )\n",
      "\u001b[1;32m/home/yanglin/Federated-Time-Series-Forecasting/notebooks/02.Individual_Training.ipynb 单元格 17\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B%E5%AE%9E%E9%AA%8C%E6%9C%BA%E5%99%A8/home/yanglin/Federated-Time-Series-Forecasting/notebooks/02.Individual_Training.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m date_time_df_val \u001b[39m=\u001b[39m time_to_feature(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B%E5%AE%9E%E9%AA%8C%E6%9C%BA%E5%99%A8/home/yanglin/Federated-Time-Series-Forecasting/notebooks/02.Individual_Training.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m     X_val, args\u001b[39m.\u001b[39muse_time_features, identifier\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39midentifier\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B%E5%AE%9E%E9%AA%8C%E6%9C%BA%E5%99%A8/home/yanglin/Federated-Time-Series-Forecasting/notebooks/02.Individual_Training.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B%E5%AE%9E%E9%AA%8C%E6%9C%BA%E5%99%A8/home/yanglin/Federated-Time-Series-Forecasting/notebooks/02.Individual_Training.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# get statistics as exogenous data\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B%E5%AE%9E%E9%AA%8C%E6%9C%BA%E5%99%A8/home/yanglin/Federated-Time-Series-Forecasting/notebooks/02.Individual_Training.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m stats_df_train \u001b[39m=\u001b[39m assign_statistics(X_train, args\u001b[39m.\u001b[39;49massign_stats, args\u001b[39m.\u001b[39;49mnum_lags,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B%E5%AE%9E%E9%AA%8C%E6%9C%BA%E5%99%A8/home/yanglin/Federated-Time-Series-Forecasting/notebooks/02.Individual_Training.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m                                    targets\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mtargets, identifier\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49midentifier)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B%E5%AE%9E%E9%AA%8C%E6%9C%BA%E5%99%A8/home/yanglin/Federated-Time-Series-Forecasting/notebooks/02.Individual_Training.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m stats_df_val \u001b[39m=\u001b[39m assign_statistics(X_val, args\u001b[39m.\u001b[39massign_stats, args\u001b[39m.\u001b[39mnum_lags, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B%E5%AE%9E%E9%AA%8C%E6%9C%BA%E5%99%A8/home/yanglin/Federated-Time-Series-Forecasting/notebooks/02.Individual_Training.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m                                    targets\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mtargets, identifier\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39midentifier)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B%E5%AE%9E%E9%AA%8C%E6%9C%BA%E5%99%A8/home/yanglin/Federated-Time-Series-Forecasting/notebooks/02.Individual_Training.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m#合并生成的外生特征（时间特征和统计特征），并去除重复的列。如果没有外生特征，则设为None。\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B%E5%AE%9E%E9%AA%8C%E6%9C%BA%E5%99%A8/home/yanglin/Federated-Time-Series-Forecasting/notebooks/02.Individual_Training.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m#exogenous_data_train 和 exogenous_data_val如果不为 None，这些是训练集和验证集的外生特征数据。\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B%E5%AE%9E%E9%AA%8C%E6%9C%BA%E5%99%A8/home/yanglin/Federated-Time-Series-Forecasting/notebooks/02.Individual_Training.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# concat the exogenous features (if any) to a single dataframe\u001b[39;00m\n",
      "File \u001b[0;32m/home/yanglin/Federated-Time-Series-Forecasting/ml/utils/data_utils.py:394\u001b[0m, in \u001b[0;36massign_statistics\u001b[0;34m(X, stats, lags, targets, identifier)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39massign_statistics\u001b[39m(X: pd\u001b[39m.\u001b[39mDataFrame,\n\u001b[1;32m    388\u001b[0m                       stats: List[\u001b[39mstr\u001b[39m],\n\u001b[1;32m    389\u001b[0m                       lags: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m,\n\u001b[1;32m    390\u001b[0m                       targets: List[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mup\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdown\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    391\u001b[0m                       identifier: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDistrict\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[pd\u001b[39m.\u001b[39mDataFrame, \u001b[39mNone\u001b[39;00m]:\n\u001b[1;32m    392\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Assigns the defined statistics as exogenous data. These statistics describe the time series used as input as a\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[39m    whole. For example, if we use 10 time lags, then the assigned statistics describe the previous 10 observations.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 394\u001b[0m     X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39;49mcopy()\n\u001b[1;32m    395\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(stats, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    396\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(stats)) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "# here exogenous_data_train and val are None.\n",
    "X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scaler, y_scaler = make_preprocessing(\n",
    "    filter_bs=\"LesCorts\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b99c6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rb_up_var_lag-10</th>\n",
       "      <th>rb_up_lag-10</th>\n",
       "      <th>rb_down_var_lag-10</th>\n",
       "      <th>rb_down_lag-10</th>\n",
       "      <th>mcs_up_var_lag-10</th>\n",
       "      <th>mcs_up_lag-10</th>\n",
       "      <th>mcs_down_var_lag-10</th>\n",
       "      <th>mcs_down_lag-10</th>\n",
       "      <th>rnti_count_lag-10</th>\n",
       "      <th>up_lag-10</th>\n",
       "      <th>...</th>\n",
       "      <th>rb_down_var_lag-1</th>\n",
       "      <th>rb_down_lag-1</th>\n",
       "      <th>mcs_up_var_lag-1</th>\n",
       "      <th>mcs_up_lag-1</th>\n",
       "      <th>mcs_down_var_lag-1</th>\n",
       "      <th>mcs_down_lag-1</th>\n",
       "      <th>rnti_count_lag-1</th>\n",
       "      <th>up_lag-1</th>\n",
       "      <th>down_lag-1</th>\n",
       "      <th>District</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:32:00</th>\n",
       "      <td>1.083682e-08</td>\n",
       "      <td>0.036054</td>\n",
       "      <td>1.961025e-08</td>\n",
       "      <td>0.109670</td>\n",
       "      <td>0.493544</td>\n",
       "      <td>0.431702</td>\n",
       "      <td>0.459601</td>\n",
       "      <td>0.160269</td>\n",
       "      <td>0.210509</td>\n",
       "      <td>0.017705</td>\n",
       "      <td>...</td>\n",
       "      <td>1.974816e-08</td>\n",
       "      <td>0.114193</td>\n",
       "      <td>0.572030</td>\n",
       "      <td>0.400005</td>\n",
       "      <td>0.460896</td>\n",
       "      <td>0.164719</td>\n",
       "      <td>0.212757</td>\n",
       "      <td>0.018888</td>\n",
       "      <td>0.131219</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:34:00</th>\n",
       "      <td>1.061491e-08</td>\n",
       "      <td>0.030426</td>\n",
       "      <td>1.644211e-08</td>\n",
       "      <td>0.098938</td>\n",
       "      <td>0.527483</td>\n",
       "      <td>0.476772</td>\n",
       "      <td>0.392094</td>\n",
       "      <td>0.131085</td>\n",
       "      <td>0.200300</td>\n",
       "      <td>0.018282</td>\n",
       "      <td>...</td>\n",
       "      <td>1.791601e-08</td>\n",
       "      <td>0.105544</td>\n",
       "      <td>0.576015</td>\n",
       "      <td>0.445876</td>\n",
       "      <td>0.427184</td>\n",
       "      <td>0.145817</td>\n",
       "      <td>0.192432</td>\n",
       "      <td>0.014424</td>\n",
       "      <td>0.113276</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:36:00</th>\n",
       "      <td>9.109702e-09</td>\n",
       "      <td>0.038901</td>\n",
       "      <td>1.761309e-08</td>\n",
       "      <td>0.106045</td>\n",
       "      <td>0.501063</td>\n",
       "      <td>0.471773</td>\n",
       "      <td>0.453949</td>\n",
       "      <td>0.152725</td>\n",
       "      <td>0.221468</td>\n",
       "      <td>0.019501</td>\n",
       "      <td>...</td>\n",
       "      <td>1.766410e-08</td>\n",
       "      <td>0.101075</td>\n",
       "      <td>0.640010</td>\n",
       "      <td>0.448971</td>\n",
       "      <td>0.387407</td>\n",
       "      <td>0.130386</td>\n",
       "      <td>0.173137</td>\n",
       "      <td>0.011194</td>\n",
       "      <td>0.101414</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:38:00</th>\n",
       "      <td>5.813802e-09</td>\n",
       "      <td>0.026050</td>\n",
       "      <td>1.668480e-08</td>\n",
       "      <td>0.098968</td>\n",
       "      <td>0.499865</td>\n",
       "      <td>0.450949</td>\n",
       "      <td>0.382516</td>\n",
       "      <td>0.129036</td>\n",
       "      <td>0.183768</td>\n",
       "      <td>0.012594</td>\n",
       "      <td>...</td>\n",
       "      <td>1.963081e-08</td>\n",
       "      <td>0.114777</td>\n",
       "      <td>0.534812</td>\n",
       "      <td>0.414812</td>\n",
       "      <td>0.476978</td>\n",
       "      <td>0.171880</td>\n",
       "      <td>0.205545</td>\n",
       "      <td>0.023585</td>\n",
       "      <td>0.141225</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:40:00</th>\n",
       "      <td>7.246522e-09</td>\n",
       "      <td>0.028981</td>\n",
       "      <td>1.721707e-08</td>\n",
       "      <td>0.102669</td>\n",
       "      <td>0.535319</td>\n",
       "      <td>0.429488</td>\n",
       "      <td>0.412994</td>\n",
       "      <td>0.139577</td>\n",
       "      <td>0.191964</td>\n",
       "      <td>0.013056</td>\n",
       "      <td>...</td>\n",
       "      <td>1.895823e-08</td>\n",
       "      <td>0.107131</td>\n",
       "      <td>0.547422</td>\n",
       "      <td>0.456544</td>\n",
       "      <td>0.448805</td>\n",
       "      <td>0.157300</td>\n",
       "      <td>0.181567</td>\n",
       "      <td>0.022127</td>\n",
       "      <td>0.125069</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     rb_up_var_lag-10  rb_up_lag-10  rb_down_var_lag-10  \\\n",
       "time                                                                      \n",
       "2019-01-12 17:32:00      1.083682e-08      0.036054        1.961025e-08   \n",
       "2019-01-12 17:34:00      1.061491e-08      0.030426        1.644211e-08   \n",
       "2019-01-12 17:36:00      9.109702e-09      0.038901        1.761309e-08   \n",
       "2019-01-12 17:38:00      5.813802e-09      0.026050        1.668480e-08   \n",
       "2019-01-12 17:40:00      7.246522e-09      0.028981        1.721707e-08   \n",
       "\n",
       "                     rb_down_lag-10  mcs_up_var_lag-10  mcs_up_lag-10  \\\n",
       "time                                                                    \n",
       "2019-01-12 17:32:00        0.109670           0.493544       0.431702   \n",
       "2019-01-12 17:34:00        0.098938           0.527483       0.476772   \n",
       "2019-01-12 17:36:00        0.106045           0.501063       0.471773   \n",
       "2019-01-12 17:38:00        0.098968           0.499865       0.450949   \n",
       "2019-01-12 17:40:00        0.102669           0.535319       0.429488   \n",
       "\n",
       "                     mcs_down_var_lag-10  mcs_down_lag-10  rnti_count_lag-10  \\\n",
       "time                                                                           \n",
       "2019-01-12 17:32:00             0.459601         0.160269           0.210509   \n",
       "2019-01-12 17:34:00             0.392094         0.131085           0.200300   \n",
       "2019-01-12 17:36:00             0.453949         0.152725           0.221468   \n",
       "2019-01-12 17:38:00             0.382516         0.129036           0.183768   \n",
       "2019-01-12 17:40:00             0.412994         0.139577           0.191964   \n",
       "\n",
       "                     up_lag-10  ...  rb_down_var_lag-1  rb_down_lag-1  \\\n",
       "time                            ...                                     \n",
       "2019-01-12 17:32:00   0.017705  ...       1.974816e-08       0.114193   \n",
       "2019-01-12 17:34:00   0.018282  ...       1.791601e-08       0.105544   \n",
       "2019-01-12 17:36:00   0.019501  ...       1.766410e-08       0.101075   \n",
       "2019-01-12 17:38:00   0.012594  ...       1.963081e-08       0.114777   \n",
       "2019-01-12 17:40:00   0.013056  ...       1.895823e-08       0.107131   \n",
       "\n",
       "                     mcs_up_var_lag-1  mcs_up_lag-1  mcs_down_var_lag-1  \\\n",
       "time                                                                      \n",
       "2019-01-12 17:32:00          0.572030      0.400005            0.460896   \n",
       "2019-01-12 17:34:00          0.576015      0.445876            0.427184   \n",
       "2019-01-12 17:36:00          0.640010      0.448971            0.387407   \n",
       "2019-01-12 17:38:00          0.534812      0.414812            0.476978   \n",
       "2019-01-12 17:40:00          0.547422      0.456544            0.448805   \n",
       "\n",
       "                     mcs_down_lag-1  rnti_count_lag-1  up_lag-1  down_lag-1  \\\n",
       "time                                                                          \n",
       "2019-01-12 17:32:00        0.164719          0.212757  0.018888    0.131219   \n",
       "2019-01-12 17:34:00        0.145817          0.192432  0.014424    0.113276   \n",
       "2019-01-12 17:36:00        0.130386          0.173137  0.011194    0.101414   \n",
       "2019-01-12 17:38:00        0.171880          0.205545  0.023585    0.141225   \n",
       "2019-01-12 17:40:00        0.157300          0.181567  0.022127    0.125069   \n",
       "\n",
       "                     District  \n",
       "time                           \n",
       "2019-01-12 17:32:00  LesCorts  \n",
       "2019-01-12 17:34:00  LesCorts  \n",
       "2019-01-12 17:36:00  LesCorts  \n",
       "2019-01-12 17:38:00  LesCorts  \n",
       "2019-01-12 17:40:00  LesCorts  \n",
       "\n",
       "[5 rows x 111 columns]"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()\n",
    "#time：时间戳，表示数据记录的具体时间\n",
    "#down：下行流量，单位可能是字节\n",
    "#up：上行流量，单位可能是字节\n",
    "#rnti_count：RNTI（Radio Network Temporary Identifier）计数，表示在指定时间内活跃的RNTI数量。\n",
    "#mcs_down：下行调制和编码方案（MCS，Modulation and Coding Scheme）的平均值，表示下行链路的调制和编码效率。\n",
    "#mcs_down_var：下行MCS的方差，表示下行链路调制和编码效率的变化程度\n",
    "#mcs_up：上行调制和编码方案（MCS，Modulation and Coding Scheme）的平均值，表示上行链路的调制和编码效率。\n",
    "#mcs_up_var：上行MCS的方差，表示上行链路调制和编码效率的变化程度\n",
    "#rb_down：下行资源块（RB，Resource Block）的平均值，表示在指定时间内下行链路使用的资源块的平均数量。\n",
    "#rb_down_var：下行资源块的方差，表示下行链路资源块使用数量的变化程度\n",
    "#rb_up：上行资源块（RB，Resource Block）的平均值，表示在指定时间内上行链路使用的资源块的平均数量\n",
    "#rb_up_var：上行资源块的方差，表示上行链路资源块使用数量的变化程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837bfd70-4962-4c1e-a8dd-4ac5e6ed261e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rb_up_var_lag-10</th>\n",
       "      <th>rb_up_lag-10</th>\n",
       "      <th>rb_down_var_lag-10</th>\n",
       "      <th>rb_down_lag-10</th>\n",
       "      <th>mcs_up_var_lag-10</th>\n",
       "      <th>mcs_up_lag-10</th>\n",
       "      <th>mcs_down_var_lag-10</th>\n",
       "      <th>mcs_down_lag-10</th>\n",
       "      <th>rnti_count_lag-10</th>\n",
       "      <th>up_lag-10</th>\n",
       "      <th>...</th>\n",
       "      <th>rb_down_var_lag-1</th>\n",
       "      <th>rb_down_lag-1</th>\n",
       "      <th>mcs_up_var_lag-1</th>\n",
       "      <th>mcs_up_lag-1</th>\n",
       "      <th>mcs_down_var_lag-1</th>\n",
       "      <th>mcs_down_lag-1</th>\n",
       "      <th>rnti_count_lag-1</th>\n",
       "      <th>up_lag-1</th>\n",
       "      <th>down_lag-1</th>\n",
       "      <th>District</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-20 09:20:00</th>\n",
       "      <td>4.728611e-09</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>4.323541e-08</td>\n",
       "      <td>0.058302</td>\n",
       "      <td>0.401278</td>\n",
       "      <td>0.501344</td>\n",
       "      <td>0.919491</td>\n",
       "      <td>0.935376</td>\n",
       "      <td>0.123355</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>...</td>\n",
       "      <td>4.402667e-08</td>\n",
       "      <td>0.072691</td>\n",
       "      <td>0.111839</td>\n",
       "      <td>0.444416</td>\n",
       "      <td>0.885758</td>\n",
       "      <td>0.931423</td>\n",
       "      <td>0.154451</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.180818</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-20 09:22:00</th>\n",
       "      <td>9.995833e-10</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>4.508320e-08</td>\n",
       "      <td>0.056214</td>\n",
       "      <td>0.159144</td>\n",
       "      <td>0.393817</td>\n",
       "      <td>0.841731</td>\n",
       "      <td>0.960886</td>\n",
       "      <td>0.118906</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>...</td>\n",
       "      <td>4.474956e-08</td>\n",
       "      <td>0.109179</td>\n",
       "      <td>0.275191</td>\n",
       "      <td>0.471308</td>\n",
       "      <td>0.877144</td>\n",
       "      <td>0.933492</td>\n",
       "      <td>0.222077</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.264679</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-20 09:24:00</th>\n",
       "      <td>1.217479e-08</td>\n",
       "      <td>0.000859</td>\n",
       "      <td>4.771216e-08</td>\n",
       "      <td>0.097153</td>\n",
       "      <td>0.301473</td>\n",
       "      <td>0.467948</td>\n",
       "      <td>0.869250</td>\n",
       "      <td>0.931095</td>\n",
       "      <td>0.202079</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>...</td>\n",
       "      <td>4.620672e-08</td>\n",
       "      <td>0.140548</td>\n",
       "      <td>0.407761</td>\n",
       "      <td>0.487723</td>\n",
       "      <td>0.900534</td>\n",
       "      <td>0.947047</td>\n",
       "      <td>0.287079</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.343279</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-20 09:26:00</th>\n",
       "      <td>9.579305e-09</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>4.462743e-08</td>\n",
       "      <td>0.100799</td>\n",
       "      <td>0.333194</td>\n",
       "      <td>0.519904</td>\n",
       "      <td>0.868349</td>\n",
       "      <td>0.948227</td>\n",
       "      <td>0.202641</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>...</td>\n",
       "      <td>4.512395e-08</td>\n",
       "      <td>0.098633</td>\n",
       "      <td>0.307474</td>\n",
       "      <td>0.474030</td>\n",
       "      <td>0.852834</td>\n",
       "      <td>0.955406</td>\n",
       "      <td>0.203906</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.241753</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-20 09:28:00</th>\n",
       "      <td>2.505653e-08</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>4.706725e-08</td>\n",
       "      <td>0.135352</td>\n",
       "      <td>0.406647</td>\n",
       "      <td>0.504277</td>\n",
       "      <td>0.900401</td>\n",
       "      <td>0.945294</td>\n",
       "      <td>0.270641</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>...</td>\n",
       "      <td>4.390479e-08</td>\n",
       "      <td>0.076930</td>\n",
       "      <td>0.214200</td>\n",
       "      <td>0.541009</td>\n",
       "      <td>0.900236</td>\n",
       "      <td>0.940772</td>\n",
       "      <td>0.160727</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.191878</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     rb_up_var_lag-10  rb_up_lag-10  rb_down_var_lag-10  \\\n",
       "time                                                                      \n",
       "2019-01-20 09:20:00      4.728611e-09      0.000378        4.323541e-08   \n",
       "2019-01-20 09:22:00      9.995833e-10      0.000330        4.508320e-08   \n",
       "2019-01-20 09:24:00      1.217479e-08      0.000859        4.771216e-08   \n",
       "2019-01-20 09:26:00      9.579305e-09      0.000850        4.462743e-08   \n",
       "2019-01-20 09:28:00      2.505653e-08      0.001541        4.706725e-08   \n",
       "\n",
       "                     rb_down_lag-10  mcs_up_var_lag-10  mcs_up_lag-10  \\\n",
       "time                                                                    \n",
       "2019-01-20 09:20:00        0.058302           0.401278       0.501344   \n",
       "2019-01-20 09:22:00        0.056214           0.159144       0.393817   \n",
       "2019-01-20 09:24:00        0.097153           0.301473       0.467948   \n",
       "2019-01-20 09:26:00        0.100799           0.333194       0.519904   \n",
       "2019-01-20 09:28:00        0.135352           0.406647       0.504277   \n",
       "\n",
       "                     mcs_down_var_lag-10  mcs_down_lag-10  rnti_count_lag-10  \\\n",
       "time                                                                           \n",
       "2019-01-20 09:20:00             0.919491         0.935376           0.123355   \n",
       "2019-01-20 09:22:00             0.841731         0.960886           0.118906   \n",
       "2019-01-20 09:24:00             0.869250         0.931095           0.202079   \n",
       "2019-01-20 09:26:00             0.868349         0.948227           0.202641   \n",
       "2019-01-20 09:28:00             0.900401         0.945294           0.270641   \n",
       "\n",
       "                     up_lag-10  ...  rb_down_var_lag-1  rb_down_lag-1  \\\n",
       "time                            ...                                     \n",
       "2019-01-20 09:20:00   0.000206  ...       4.402667e-08       0.072691   \n",
       "2019-01-20 09:22:00   0.000160  ...       4.474956e-08       0.109179   \n",
       "2019-01-20 09:24:00   0.000558  ...       4.620672e-08       0.140548   \n",
       "2019-01-20 09:26:00   0.000568  ...       4.512395e-08       0.098633   \n",
       "2019-01-20 09:28:00   0.001207  ...       4.390479e-08       0.076930   \n",
       "\n",
       "                     mcs_up_var_lag-1  mcs_up_lag-1  mcs_down_var_lag-1  \\\n",
       "time                                                                      \n",
       "2019-01-20 09:20:00          0.111839      0.444416            0.885758   \n",
       "2019-01-20 09:22:00          0.275191      0.471308            0.877144   \n",
       "2019-01-20 09:24:00          0.407761      0.487723            0.900534   \n",
       "2019-01-20 09:26:00          0.307474      0.474030            0.852834   \n",
       "2019-01-20 09:28:00          0.214200      0.541009            0.900236   \n",
       "\n",
       "                     mcs_down_lag-1  rnti_count_lag-1  up_lag-1  down_lag-1  \\\n",
       "time                                                                          \n",
       "2019-01-20 09:20:00        0.931423          0.154451  0.000336    0.180818   \n",
       "2019-01-20 09:22:00        0.933492          0.222077  0.000623    0.264679   \n",
       "2019-01-20 09:24:00        0.947047          0.287079  0.000887    0.343279   \n",
       "2019-01-20 09:26:00        0.955406          0.203906  0.000384    0.241753   \n",
       "2019-01-20 09:28:00        0.940772          0.160727  0.000374    0.191878   \n",
       "\n",
       "                     District  \n",
       "time                           \n",
       "2019-01-20 09:20:00  LesCorts  \n",
       "2019-01-20 09:22:00  LesCorts  \n",
       "2019-01-20 09:24:00  LesCorts  \n",
       "2019-01-20 09:26:00  LesCorts  \n",
       "2019-01-20 09:28:00  LesCorts  \n",
       "\n",
       "[5 rows x 111 columns]"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#解释\n",
    "X_val.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec22537f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rnti_count</th>\n",
       "      <th>rb_down</th>\n",
       "      <th>rb_up</th>\n",
       "      <th>down</th>\n",
       "      <th>up</th>\n",
       "      <th>District</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:32:00</th>\n",
       "      <td>0.192432</td>\n",
       "      <td>0.105544</td>\n",
       "      <td>0.033785</td>\n",
       "      <td>0.113276</td>\n",
       "      <td>0.014424</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:34:00</th>\n",
       "      <td>0.173137</td>\n",
       "      <td>0.101075</td>\n",
       "      <td>0.025216</td>\n",
       "      <td>0.101414</td>\n",
       "      <td>0.011194</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:36:00</th>\n",
       "      <td>0.205545</td>\n",
       "      <td>0.114777</td>\n",
       "      <td>0.060088</td>\n",
       "      <td>0.141225</td>\n",
       "      <td>0.023585</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:38:00</th>\n",
       "      <td>0.181567</td>\n",
       "      <td>0.107131</td>\n",
       "      <td>0.042592</td>\n",
       "      <td>0.125069</td>\n",
       "      <td>0.022127</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:40:00</th>\n",
       "      <td>0.175900</td>\n",
       "      <td>0.101726</td>\n",
       "      <td>0.023463</td>\n",
       "      <td>0.107920</td>\n",
       "      <td>0.011213</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     rnti_count   rb_down     rb_up      down        up  \\\n",
       "time                                                                      \n",
       "2019-01-12 17:32:00    0.192432  0.105544  0.033785  0.113276  0.014424   \n",
       "2019-01-12 17:34:00    0.173137  0.101075  0.025216  0.101414  0.011194   \n",
       "2019-01-12 17:36:00    0.205545  0.114777  0.060088  0.141225  0.023585   \n",
       "2019-01-12 17:38:00    0.181567  0.107131  0.042592  0.125069  0.022127   \n",
       "2019-01-12 17:40:00    0.175900  0.101726  0.023463  0.107920  0.011213   \n",
       "\n",
       "                     District  \n",
       "time                           \n",
       "2019-01-12 17:32:00  LesCorts  \n",
       "2019-01-12 17:34:00  LesCorts  \n",
       "2019-01-12 17:36:00  LesCorts  \n",
       "2019-01-12 17:38:00  LesCorts  \n",
       "2019-01-12 17:40:00  LesCorts  "
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367c2b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MinMaxScaler(), MinMaxScaler())"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaler, y_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488a9c30",
   "metadata": {},
   "source": [
    "### Postprocessing Stage\n",
    "\n",
    "In this stage we transform data in a way that can be fed into ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c417082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_postprocessing(X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scaler, y_scaler):\n",
    "    #X_train, X_val: 训练集和验证集的特征数据。\n",
    "    #y_train, y_val: 训练集和验证集的目标数据。\n",
    "    #exogenous_data_train, exogenous_data_val: 训练集和验证集的外生数据。\n",
    "    #x_scaler, y_scaler: 特征数据和目标数据的缩放器。\n",
    "    \"\"\"Make data ready to be fed into ml algorithms\"\"\"\n",
    "    # if there are more than one specified areas, get the data per area \n",
    "    #检查训练数据集中是否有多个不同的区域（由 args.identifier 标识）。如果有多个区域，则调用 get_data_by_area 函数将数据按区域分割。\n",
    "    if X_train[args.identifier].nunique() != 1:\n",
    "        area_X_train, area_X_val, area_y_train, area_y_val = get_data_by_area(X_train, X_val,\n",
    "                                                                              y_train, y_val, \n",
    "                                                                              identifier=args.identifier)\n",
    "    else:\n",
    "        area_X_train, area_X_val, area_y_train, area_y_val = None, None, None, None\n",
    "\n",
    "    # Get the exogenous data per area. 获取每个区域的外生数据 如果存在外生数据，则将其按区域分割。\n",
    "    if exogenous_data_train is not None:\n",
    "        exogenous_data_train, exogenous_data_val = get_exogenous_data_by_area(exogenous_data_train,\n",
    "                                                                              exogenous_data_val)\n",
    "    # transform to np 将区域划分的数据转换为 NumPy 数组，并移除标识符列（如区域名称）。\n",
    "    if area_X_train is not None:\n",
    "        for area in area_X_train:\n",
    "            tmp_X_train, tmp_y_train, tmp_X_val, tmp_y_val = remove_identifiers(\n",
    "                area_X_train[area], area_y_train[area], area_X_val[area], area_y_val[area])\n",
    "            tmp_X_train, tmp_y_train = tmp_X_train.to_numpy(), tmp_y_train.to_numpy()\n",
    "            tmp_X_val, tmp_y_val = tmp_X_val.to_numpy(), tmp_y_val.to_numpy()\n",
    "            area_X_train[area] = tmp_X_train\n",
    "            area_X_val[area] = tmp_X_val\n",
    "            area_y_train[area] = tmp_y_train\n",
    "            area_y_val[area] = tmp_y_val\n",
    "    \n",
    "    if exogenous_data_train is not None:\n",
    "        for area in exogenous_data_train:\n",
    "            exogenous_data_train[area] = exogenous_data_train[area].to_numpy()\n",
    "            exogenous_data_val[area] = exogenous_data_val[area].to_numpy()\n",
    "    \n",
    "    # remove identifiers from features, targets 对整体数据集，移除标识符列，并确保训练集和验证集的特征列数相同。\n",
    "    X_train, y_train, X_val, y_val = remove_identifiers(X_train, y_train, X_val, y_val)\n",
    "    assert len(X_train.columns) == len(X_val.columns)\n",
    "    #计算特征数量，这里每个特征都有多个滞后期，所以除以滞后期的数量\n",
    "    num_features = len(X_train.columns) // args.num_lags\n",
    "    \n",
    "    # to timeseries representation 将特征数据转换为时间序列表示，即构建时间滞后的特征集\n",
    "    X_train = to_timeseries_rep(X_train.to_numpy(), num_lags=args.num_lags,\n",
    "                                            num_features=num_features)\n",
    "    X_val = to_timeseries_rep(X_val.to_numpy(), num_lags=args.num_lags,\n",
    "                                          num_features=num_features)\n",
    "    \n",
    "    if area_X_train is not None:\n",
    "        area_X_train = to_timeseries_rep(area_X_train, num_lags=args.num_lags,\n",
    "                                                     num_features=num_features)\n",
    "        area_X_val = to_timeseries_rep(area_X_val, num_lags=args.num_lags,\n",
    "                                                   num_features=num_features)\n",
    "    \n",
    "    # transform targets to numpy 将目标数据转换为 NumPy 数组\n",
    "    y_train, y_val = y_train.to_numpy(), y_val.to_numpy()\n",
    "    \n",
    "    # centralized (all) learning specific 在集中学习的情况下，将所有区域的外生数据合并成一个数据集\n",
    "    if not args.filter_bs and exogenous_data_train is not None:\n",
    "        exogenous_data_train_combined, exogenous_data_val_combined = [], []\n",
    "        for area in exogenous_data_train:\n",
    "            exogenous_data_train_combined.extend(exogenous_data_train[area])\n",
    "            exogenous_data_val_combined.extend(exogenous_data_val[area])\n",
    "        exogenous_data_train_combined = np.stack(exogenous_data_train_combined)\n",
    "        exogenous_data_val_combined = np.stack(exogenous_data_val_combined)\n",
    "        exogenous_data_train[\"all\"] = exogenous_data_train_combined\n",
    "        exogenous_data_val[\"all\"] = exogenous_data_val_combined\n",
    "    return X_train, X_val, y_train, y_val, area_X_train, area_X_val, area_y_train, area_y_val, exogenous_data_train, exogenous_data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c59dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val, area_X_train, area_X_val, area_y_train, area_y_val, exogenous_data_train, exogenous_data_val = make_postprocessing(X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scaler, y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9171667b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[1.08368177e-08],\n",
       "         [3.60536501e-02],\n",
       "         [1.96102530e-08],\n",
       "         [1.09670356e-01],\n",
       "         [4.93544310e-01],\n",
       "         [4.31701750e-01],\n",
       "         [4.59600508e-01],\n",
       "         [1.60269260e-01],\n",
       "         [2.10509062e-01],\n",
       "         [1.77047513e-02],\n",
       "         [1.29703119e-01]],\n",
       "\n",
       "        [[1.06149072e-08],\n",
       "         [3.04260999e-02],\n",
       "         [1.64421117e-08],\n",
       "         [9.89375487e-02],\n",
       "         [5.27482629e-01],\n",
       "         [4.76772249e-01],\n",
       "         [3.92093986e-01],\n",
       "         [1.31084532e-01],\n",
       "         [2.00299725e-01],\n",
       "         [1.82819255e-02],\n",
       "         [9.46676582e-02]],\n",
       "\n",
       "        [[9.10970233e-09],\n",
       "         [3.89005616e-02],\n",
       "         [1.76130861e-08],\n",
       "         [1.06044851e-01],\n",
       "         [5.01062572e-01],\n",
       "         [4.71772611e-01],\n",
       "         [4.53948647e-01],\n",
       "         [1.52724519e-01],\n",
       "         [2.21467718e-01],\n",
       "         [1.95014086e-02],\n",
       "         [1.11105040e-01]],\n",
       "\n",
       "        [[5.81380233e-09],\n",
       "         [2.60500330e-02],\n",
       "         [1.66847958e-08],\n",
       "         [9.89684388e-02],\n",
       "         [4.99864906e-01],\n",
       "         [4.50949341e-01],\n",
       "         [3.82516414e-01],\n",
       "         [1.29036337e-01],\n",
       "         [1.83768094e-01],\n",
       "         [1.25944829e-02],\n",
       "         [9.68663320e-02]],\n",
       "\n",
       "        [[7.24652205e-09],\n",
       "         [2.89812144e-02],\n",
       "         [1.72170704e-08],\n",
       "         [1.02669150e-01],\n",
       "         [5.35318553e-01],\n",
       "         [4.29487944e-01],\n",
       "         [4.12994266e-01],\n",
       "         [1.39576867e-01],\n",
       "         [1.91963658e-01],\n",
       "         [1.30563863e-02],\n",
       "         [1.04084052e-01]],\n",
       "\n",
       "        [[6.06154105e-09],\n",
       "         [4.28037606e-02],\n",
       "         [2.31000232e-08],\n",
       "         [1.32117063e-01],\n",
       "         [5.78713536e-01],\n",
       "         [4.60009694e-01],\n",
       "         [5.32073081e-01],\n",
       "         [1.96870178e-01],\n",
       "         [2.43010357e-01],\n",
       "         [1.87698063e-02],\n",
       "         [1.75482631e-01]],\n",
       "\n",
       "        [[6.05376371e-09],\n",
       "         [3.68960202e-02],\n",
       "         [2.13197904e-08],\n",
       "         [1.28368676e-01],\n",
       "         [6.27172947e-01],\n",
       "         [4.36633140e-01],\n",
       "         [5.26264966e-01],\n",
       "         [1.94025993e-01],\n",
       "         [2.43150860e-01],\n",
       "         [1.70038119e-02],\n",
       "         [1.67414486e-01]],\n",
       "\n",
       "        [[1.08545919e-08],\n",
       "         [4.05925028e-02],\n",
       "         [1.91197547e-08],\n",
       "         [1.09047189e-01],\n",
       "         [5.68330526e-01],\n",
       "         [3.95149767e-01],\n",
       "         [4.69213665e-01],\n",
       "         [1.62352383e-01],\n",
       "         [2.05591723e-01],\n",
       "         [1.58412419e-02],\n",
       "         [1.19397290e-01]],\n",
       "\n",
       "        [[1.06866844e-08],\n",
       "         [1.01436771e-01],\n",
       "         [1.97316883e-08],\n",
       "         [1.14744321e-01],\n",
       "         [4.81375158e-01],\n",
       "         [3.61676991e-01],\n",
       "         [4.93533880e-01],\n",
       "         [1.76247209e-01],\n",
       "         [2.22966328e-01],\n",
       "         [2.71484163e-02],\n",
       "         [1.40494898e-01]],\n",
       "\n",
       "        [[1.21875967e-08],\n",
       "         [4.45951000e-02],\n",
       "         [1.97481569e-08],\n",
       "         [1.14193007e-01],\n",
       "         [5.72030365e-01],\n",
       "         [4.00004745e-01],\n",
       "         [4.60895538e-01],\n",
       "         [1.64718658e-01],\n",
       "         [2.12756991e-01],\n",
       "         [1.88877694e-02],\n",
       "         [1.31219164e-01]]],\n",
       "\n",
       "\n",
       "       [[[1.06149072e-08],\n",
       "         [3.04260999e-02],\n",
       "         [1.64421117e-08],\n",
       "         [9.89375487e-02],\n",
       "         [5.27482629e-01],\n",
       "         [4.76772249e-01],\n",
       "         [3.92093986e-01],\n",
       "         [1.31084532e-01],\n",
       "         [2.00299725e-01],\n",
       "         [1.82819255e-02],\n",
       "         [9.46676582e-02]],\n",
       "\n",
       "        [[9.10970233e-09],\n",
       "         [3.89005616e-02],\n",
       "         [1.76130861e-08],\n",
       "         [1.06044851e-01],\n",
       "         [5.01062572e-01],\n",
       "         [4.71772611e-01],\n",
       "         [4.53948647e-01],\n",
       "         [1.52724519e-01],\n",
       "         [2.21467718e-01],\n",
       "         [1.95014086e-02],\n",
       "         [1.11105040e-01]],\n",
       "\n",
       "        [[5.81380233e-09],\n",
       "         [2.60500330e-02],\n",
       "         [1.66847958e-08],\n",
       "         [9.89684388e-02],\n",
       "         [4.99864906e-01],\n",
       "         [4.50949341e-01],\n",
       "         [3.82516414e-01],\n",
       "         [1.29036337e-01],\n",
       "         [1.83768094e-01],\n",
       "         [1.25944829e-02],\n",
       "         [9.68663320e-02]],\n",
       "\n",
       "        [[7.24652205e-09],\n",
       "         [2.89812144e-02],\n",
       "         [1.72170704e-08],\n",
       "         [1.02669150e-01],\n",
       "         [5.35318553e-01],\n",
       "         [4.29487944e-01],\n",
       "         [4.12994266e-01],\n",
       "         [1.39576867e-01],\n",
       "         [1.91963658e-01],\n",
       "         [1.30563863e-02],\n",
       "         [1.04084052e-01]],\n",
       "\n",
       "        [[6.06154105e-09],\n",
       "         [4.28037606e-02],\n",
       "         [2.31000232e-08],\n",
       "         [1.32117063e-01],\n",
       "         [5.78713536e-01],\n",
       "         [4.60009694e-01],\n",
       "         [5.32073081e-01],\n",
       "         [1.96870178e-01],\n",
       "         [2.43010357e-01],\n",
       "         [1.87698063e-02],\n",
       "         [1.75482631e-01]],\n",
       "\n",
       "        [[6.05376371e-09],\n",
       "         [3.68960202e-02],\n",
       "         [2.13197904e-08],\n",
       "         [1.28368676e-01],\n",
       "         [6.27172947e-01],\n",
       "         [4.36633140e-01],\n",
       "         [5.26264966e-01],\n",
       "         [1.94025993e-01],\n",
       "         [2.43150860e-01],\n",
       "         [1.70038119e-02],\n",
       "         [1.67414486e-01]],\n",
       "\n",
       "        [[1.08545919e-08],\n",
       "         [4.05925028e-02],\n",
       "         [1.91197547e-08],\n",
       "         [1.09047189e-01],\n",
       "         [5.68330526e-01],\n",
       "         [3.95149767e-01],\n",
       "         [4.69213665e-01],\n",
       "         [1.62352383e-01],\n",
       "         [2.05591723e-01],\n",
       "         [1.58412419e-02],\n",
       "         [1.19397290e-01]],\n",
       "\n",
       "        [[1.06866844e-08],\n",
       "         [1.01436771e-01],\n",
       "         [1.97316883e-08],\n",
       "         [1.14744321e-01],\n",
       "         [4.81375158e-01],\n",
       "         [3.61676991e-01],\n",
       "         [4.93533880e-01],\n",
       "         [1.76247209e-01],\n",
       "         [2.22966328e-01],\n",
       "         [2.71484163e-02],\n",
       "         [1.40494898e-01]],\n",
       "\n",
       "        [[1.21875967e-08],\n",
       "         [4.45951000e-02],\n",
       "         [1.97481569e-08],\n",
       "         [1.14193007e-01],\n",
       "         [5.72030365e-01],\n",
       "         [4.00004745e-01],\n",
       "         [4.60895538e-01],\n",
       "         [1.64718658e-01],\n",
       "         [2.12756991e-01],\n",
       "         [1.88877694e-02],\n",
       "         [1.31219164e-01]],\n",
       "\n",
       "        [[9.16101239e-09],\n",
       "         [3.37851271e-02],\n",
       "         [1.79160082e-08],\n",
       "         [1.05543904e-01],\n",
       "         [5.76015115e-01],\n",
       "         [4.45875734e-01],\n",
       "         [4.27184075e-01],\n",
       "         [1.45816624e-01],\n",
       "         [1.92431986e-01],\n",
       "         [1.44237261e-02],\n",
       "         [1.13276407e-01]]]])"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60d4f55",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2949d425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19243199, 0.1055439 , 0.03378513, 0.11327641, 0.01442373],\n",
       "       [0.17313728, 0.10107498, 0.02521631, 0.10141373, 0.01119418]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7bb251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5504, 1368)"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a83357",
   "metadata": {},
   "source": [
    "### Define the input dimensions for the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1486173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_dims(X_train, exogenous_data_train): \n",
    "    #计算模型输入的维度，X_train: 训练集的主输入数据（通常是一个多维数组）；exogenous_data_train: 训练集的外生数据（可以是 None 或包含外生特征的字典）。\n",
    "    if args.model_name == \"mlp\":\n",
    "        input_dim = X_train.shape[1] * X_train.shape[2] #如果模型是多层感知机（MLP），则将输入的所有特征展平成一个一维向量，因此输入维度是 X_train 的第二维度和第三维度的乘积（即 X_train.shape[1] * X_train.shape[2]）\n",
    "    else:\n",
    "        input_dim = X_train.shape[2] #对于其他模型（如卷积神经网络或循环神经网络），输入维度保持为 X_train 的第三维度（即 X_train.shape[2]）\n",
    "    #计算外生数据的维度\n",
    "    if exogenous_data_train is not None: #如果 exogenous_data_train 不为 None\n",
    "        if len(exogenous_data_train) == 1: #如果外生数据中只有一个区域\n",
    "            cid = next(iter(exogenous_data_train.keys()))\n",
    "            exogenous_dim = exogenous_data_train[cid].shape[1] #从字典中获取该区域的维度。\n",
    "        else:\n",
    "            exogenous_dim = exogenous_data_train[\"all\"].shape[1] #如果外生数据中有多个区域，则使用键为 \"all\" 的区域的维度\n",
    "    else:\n",
    "        exogenous_dim = 0 #如果 exogenous_data_train 为 None，则外生数据的维度为 0\n",
    "    \n",
    "    return input_dim, exogenous_dim #input_dim: 主输入数据的维度；exogenous_dim: 外生数据的维度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6783ff",
   "metadata": {},
   "source": [
    "### Initialize the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7044d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model: str,\n",
    "              input_dim: int,\n",
    "              out_dim: int,\n",
    "              lags: int = 10,\n",
    "              exogenous_dim: int = 0,\n",
    "              seed=0):\n",
    "    if model == \"mlp\":\n",
    "        model = MLP(input_dim=input_dim, layer_units=[256, 128, 64], num_outputs=out_dim)\n",
    "    elif model == \"rnn\":\n",
    "        model = RNN(input_dim=input_dim, rnn_hidden_size=128, num_rnn_layers=1, rnn_dropout=0.0,\n",
    "                    layer_units=[128], num_outputs=out_dim, matrix_rep=True, exogenous_dim=exogenous_dim)\n",
    "    elif model == \"lstm\":\n",
    "        model = LSTM(input_dim=input_dim, lstm_hidden_size=128, num_lstm_layers=1, lstm_dropout=0.0,\n",
    "                     layer_units=[128], num_outputs=out_dim, matrix_rep=True, exogenous_dim=exogenous_dim)\n",
    "    elif model == \"gru\":\n",
    "        model = GRU(input_dim=input_dim, gru_hidden_size=128, num_gru_layers=1, gru_dropout=0.0,\n",
    "                    layer_units=[128], num_outputs=out_dim, matrix_rep=True, exogenous_dim=exogenous_dim)\n",
    "    elif model == \"cnn\":\n",
    "        model = CNN(num_features=input_dim, lags=lags, exogenous_dim=exogenous_dim, out_dim=out_dim)\n",
    "    elif model == \"da_encoder_decoder\":\n",
    "        model = DualAttentionAutoEncoder(input_dim=input_dim, architecture=\"lstm\", matrix_rep=True)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Specified model is not implemented. Plese define your own model or choose one from ['mlp', 'rnn', 'lstm', 'gru', 'cnn', 'da_encoder_decoder']\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ed1dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 0\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "args.model_name = \"mlp\"\n",
    "\n",
    "input_dim, exogenous_dim = get_input_dims(X_train, exogenous_data_train)\n",
    "\n",
    "print(input_dim, exogenous_dim)\n",
    "\n",
    "model = get_model(model=args.model_name,\n",
    "                  input_dim=input_dim,\n",
    "                  out_dim=y_train.shape[1],\n",
    "                  lags=args.num_lags,\n",
    "                  exogenous_dim=exogenous_dim,\n",
    "                  seed=args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f285c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (MLP_layers): Sequential(\n",
       "    (0): Linear(in_features=110, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=64, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2473d72",
   "metadata": {},
   "source": [
    "### The fit function used to train the model specified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b11fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, X_train, y_train, X_val, y_val, #model: 需要训练的模型；X_train, y_train: 训练数据和标签；X_val, y_val: 验证数据和标签\n",
    "        exogenous_data_train=None, exogenous_data_val=None, #exogenous_data_train, exogenous_data_val: 训练和验证数据的外生特征\n",
    "        idxs=[8, 3, 1, 10, 9], # the indices of our targets in X ；idxs: 目标变量在输入数据中的索引\n",
    "        log_per=1): #log_per: 记录训练日志的频率\n",
    "    \n",
    "    # get exogenous data (if any)\n",
    "    if exogenous_data_train is not None and len(exogenous_data_train) > 1:\n",
    "        exogenous_data_train = exogenous_data_train[\"all\"]\n",
    "        exogenous_data_val = exogenous_data_val[\"all\"]\n",
    "    elif exogenous_data_train is not None and len(exogenous_data_train) == 1:\n",
    "        cid = next(iter(exogenous_data_train.keys()))\n",
    "        exogenous_data_train = exogenous_data_train[cid]\n",
    "        exogenous_data_val = exogenous_data_val[cid]\n",
    "    else:\n",
    "        exogenous_data_train = None\n",
    "        exogenous_data_val = None\n",
    "    num_features = len(X_train[0][0]) #计算特征数量，X_train 是一个 3D 数组，形状为 (num_samples, num_lags, num_features)\n",
    "    \n",
    "    # to torch loader\n",
    "    train_loader = to_torch_dataset(X_train, y_train,\n",
    "                                    num_lags=args.num_lags,\n",
    "                                    num_features=num_features,\n",
    "                                    exogenous_data=exogenous_data_train,\n",
    "                                    indices=idxs,\n",
    "                                    batch_size=args.batch_size, \n",
    "                                    shuffle=False)\n",
    "    val_loader = to_torch_dataset(X_val, y_val, \n",
    "                                  num_lags=args.num_lags,\n",
    "                                  num_features=num_features,\n",
    "                                  exogenous_data=exogenous_data_val,\n",
    "                                  indices=idxs,\n",
    "                                  batch_size=args.batch_size,\n",
    "                                  shuffle=False)\n",
    "    \n",
    "    # train the model\n",
    "    model = train(model, \n",
    "                  train_loader, val_loader,\n",
    "                  epochs=args.epochs,\n",
    "                  optimizer=args.optimizer, lr=args.lr,\n",
    "                  criterion=args.criterion,\n",
    "                  early_stopping=args.early_stopping,\n",
    "                  patience=args.patience,\n",
    "                  plot_history=args.plot_history, \n",
    "                  device=device, log_per=log_per)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "681cc512",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/yanglin/Federated-Time-Series-Forecasting/notebooks/02.Individual_Training.ipynb 单元格 37\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B%E5%AE%9E%E9%AA%8C%E6%9C%BA%E5%99%A8/home/yanglin/Federated-Time-Series-Forecasting/notebooks/02.Individual_Training.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trained_model \u001b[39m=\u001b[39m fit(model, X_train, y_train, X_val, y_val)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fit' is not defined"
     ]
    }
   ],
   "source": [
    "trained_model = fit(model, X_train, y_train, X_val, y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "a39106e1a9d6d153b7400628e7589ff266b5caee5b0db427f0903be982155882"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
